{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpPnin/EbuvPs+WrGucoeY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSharon/Arona/blob/main/Arona.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Core Model Architecture"
      ],
      "metadata": {
        "id": "EKPwUrUwJs25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RelativePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=10000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.pe = self._create_relative_positional_encoding()\n",
        "\n",
        "    def _create_relative_positional_encoding(self):\n",
        "        # Implementation of relative positional encoding for extended context\n",
        "        position = torch.arange(0, self.max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * -(math.log(10000.0) / self.d_model))\n",
        "        pe = torch.zeros(self.max_len, self.d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply relative positional encoding\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "        relative_positions = torch.arange(-seq_len+1, seq_len).unsqueeze(0).expand(batch_size, -1)\n",
        "        relative_positions = torch.clamp(relative_positions, 0, self.max_len-1)\n",
        "        return x + self.pe[relative_positions]\n",
        "\n",
        "class ExtendedAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.o_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None, rel_pos=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Project to multi-head\n",
        "        q = self.q_proj(q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(k).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(v).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention with relative positional encoding\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Add relative positional bias if provided\n",
        "        if rel_pos is not None:\n",
        "            scores = scores + rel_pos\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attention, v)\n",
        "\n",
        "        # Reshape back to batch_size x seq_len x d_model\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.o_proj(context)\n",
        "\n",
        "        return output, attention\n",
        "\n",
        "class HierarchicalMemoryBlock(nn.Module):\n",
        "    def __init__(self, d_model, memory_size=1000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.memory_size = memory_size\n",
        "\n",
        "        # Memory stores\n",
        "        self.active_memory = nn.Parameter(torch.zeros(memory_size, d_model))\n",
        "        self.short_term_memory = nn.Parameter(torch.zeros(memory_size, d_model))\n",
        "        self.long_term_memory = nn.Parameter(torch.zeros(memory_size, d_model))\n",
        "\n",
        "        # Query projections\n",
        "        self.query_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Memory compression\n",
        "        self.compressor = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model // 2, d_model)\n",
        "        )\n",
        "\n",
        "    def store_memory(self, input_data, memory_type=\"active\"):\n",
        "        # Compress and store new information\n",
        "        compressed = self.compressor(input_data)\n",
        "\n",
        "        if memory_type == \"active\":\n",
        "            # Shift active memory and add new data\n",
        "            self.active_memory = torch.cat([compressed, self.active_memory[:-compressed.size(0)]], dim=0)\n",
        "        elif memory_type == \"short_term\":\n",
        "            # Store important information from active to short-term\n",
        "            importance = self._calculate_importance(input_data)\n",
        "            mask = importance > 0.5  # Threshold for importance\n",
        "            self.short_term_memory = torch.cat(\n",
        "                [compressed[mask], self.short_term_memory[:-torch.sum(mask).item()]], dim=0\n",
        "            )\n",
        "        elif memory_type == \"long_term\":\n",
        "            # Store critical information to long-term\n",
        "            importance = self._calculate_importance(input_data)\n",
        "            mask = importance > 0.8  # Higher threshold for long-term\n",
        "            self.long_term_memory = torch.cat(\n",
        "                [compressed[mask], self.long_term_memory[:-torch.sum(mask).item()]], dim=0\n",
        "            )\n",
        "\n",
        "    def _calculate_importance(self, data):\n",
        "        # Calculate importance score for memory items\n",
        "        # This could be based on emotional salience, novelty, etc.\n",
        "        norm = torch.norm(data, dim=-1)\n",
        "        return torch.sigmoid(norm - 5.0)  # Example threshold\n",
        "\n",
        "    def retrieve_memory(self, query, top_k=5):\n",
        "        # Project query\n",
        "        query_proj = self.query_proj(query)\n",
        "\n",
        "        # Calculate relevance scores\n",
        "        active_scores = F.cosine_similarity(query_proj.unsqueeze(1), self.active_memory.unsqueeze(0), dim=-1)\n",
        "        short_term_scores = F.cosine_similarity(query_proj.unsqueeze(1), self.short_term_memory.unsqueeze(0), dim=-1)\n",
        "        long_term_scores = F.cosine_similarity(query_proj.unsqueeze(1), self.long_term_memory.unsqueeze(0), dim=-1)\n",
        "\n",
        "        # Combine scores with decreasing weights for further memories\n",
        "        combined_memories = torch.cat([\n",
        "            self.active_memory * 1.0,\n",
        "            self.short_term_memory * 0.7,\n",
        "            self.long_term_memory * 0.5\n",
        "        ], dim=0)\n",
        "\n",
        "        combined_scores = torch.cat([\n",
        "            active_scores * 1.0,\n",
        "            short_term_scores * 0.7,\n",
        "            long_term_scores * 0.5\n",
        "        ], dim=1)\n",
        "\n",
        "        # Get top-k memories\n",
        "        top_k_scores, top_k_indices = torch.topk(combined_scores, k=top_k, dim=1)\n",
        "        top_k_weights = F.softmax(top_k_scores, dim=1)\n",
        "\n",
        "        # Retrieve and weight memories\n",
        "        batch_size = query.size(0)\n",
        "        retrieved_memories = torch.zeros(batch_size, self.d_model, device=query.device)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            selected_memories = combined_memories[top_k_indices[i]]\n",
        "            retrieved_memories[i] = torch.sum(selected_memories * top_k_weights[i].unsqueeze(1), dim=0)\n",
        "\n",
        "        return retrieved_memories"
      ],
      "metadata": {
        "id": "aiDsOnPGN5Nk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tranining Architecture\n"
      ],
      "metadata": {
        "id": "e6FAuuHJN_8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiObjectiveLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, beta=0.5, gamma=0.3):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # Weight for language modeling\n",
        "        self.beta = beta    # Weight for context retention\n",
        "        self.gamma = gamma  # Weight for conversation structure\n",
        "\n",
        "    def forward(self, lm_loss, context_loss, conv_loss):\n",
        "        return self.alpha * lm_loss + self.beta * context_loss + self.gamma * conv_loss\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Get data\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        context_ids = batch[\"context_ids\"].to(device)  # Additional context information\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            context_ids=context_ids\n",
        "        )\n",
        "\n",
        "        # Calculate losses\n",
        "        lm_loss = F.cross_entropy(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
        "        context_loss = calculate_context_retention_loss(outputs.context_predictions, batch[\"context_labels\"].to(device))\n",
        "        conv_loss = calculate_conversation_structure_loss(outputs.conv_predictions, batch[\"conv_labels\"].to(device))\n",
        "\n",
        "        # Combined loss\n",
        "        criterion = MultiObjectiveLoss(alpha=1.0, beta=0.5, gamma=0.3)\n",
        "        loss = criterion(lm_loss, context_loss, conv_loss)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Distributed training setup\n",
        "def setup_distributed_training(model, world_size):\n",
        "    \"\"\"Set up model for distributed training\"\"\"\n",
        "    # Model parallelism - split layers across GPUs\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model)\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    return model, scaler"
      ],
      "metadata": {
        "id": "MaQy6NxXOH_U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emotional Intellegence Componenet"
      ],
      "metadata": {
        "id": "tI49zmSLOYsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionRecognitionModule(nn.Module):\n",
        "    def __init__(self, d_model, num_emotions=8):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_emotions = num_emotions\n",
        "\n",
        "        # Token-level emotion detection\n",
        "        self.token_emotion = nn.Linear(d_model, num_emotions)\n",
        "\n",
        "        # Utterance-level aggregation\n",
        "        self.query_emotion = nn.Parameter(torch.randn(d_model))\n",
        "\n",
        "        # Conversation-level tracking\n",
        "        self.emotion_gru = nn.GRU(num_emotions, num_emotions, batch_first=True)\n",
        "\n",
        "        # Dimensional emotion representation (valence, arousal, dominance)\n",
        "        self.emotion_dimensions = nn.Linear(d_model, 3)\n",
        "\n",
        "    def forward(self, hidden_states, utterance_boundaries=None):\n",
        "        batch_size, seq_len = hidden_states.size(0), hidden_states.size(1)\n",
        "\n",
        "        # Token-level emotion\n",
        "        token_emotions = self.token_emotion(hidden_states)  # [batch, seq, num_emotions]\n",
        "\n",
        "        # Utterance-level emotion with attention\n",
        "        query = self.query_emotion.expand(batch_size, 1, self.d_model)\n",
        "        attention_scores = torch.bmm(query, hidden_states.transpose(1, 2)).squeeze(1)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(1)\n",
        "        utterance_emotion = torch.bmm(attention_weights, token_emotions).squeeze(1)\n",
        "\n",
        "        # Split by utterance boundaries if provided\n",
        "        if utterance_boundaries is not None:\n",
        "            utterance_emotions = []\n",
        "            for b in range(batch_size):\n",
        "                # Extract utterances based on boundaries\n",
        "                boundaries = utterance_boundaries[b]\n",
        "                batch_utterances = []\n",
        "\n",
        "                for i in range(len(boundaries) - 1):\n",
        "                    start, end = boundaries[i], boundaries[i+1]\n",
        "                    # Average emotions for this utterance\n",
        "                    utterance_avg = token_emotions[b, start:end].mean(dim=0)\n",
        "                    batch_utterances.append(utterance_avg)\n",
        "\n",
        "                utterance_emotions.append(torch.stack(batch_utterances))\n",
        "\n",
        "            # Pad to same length\n",
        "            max_utterances = max(len(u) for u in utterance_emotions)\n",
        "            padded_emotions = torch.zeros(batch_size, max_utterances, self.num_emotions, device=hidden_states.device)\n",
        "            for b, emotions in enumerate(utterance_emotions):\n",
        "                padded_emotions[b, :len(emotions)] = emotions\n",
        "\n",
        "            # Conversation-level emotion tracking with GRU\n",
        "            conversation_emotions, _ = self.emotion_gru(padded_emotions)\n",
        "        else:\n",
        "            # If no boundaries, treat sequence as one utterance\n",
        "            conversation_emotions = utterance_emotion.unsqueeze(1)\n",
        "\n",
        "        # Dimensional emotion representation\n",
        "        emotion_dims = self.emotion_dimensions(hidden_states)  # [batch, seq, 3]\n",
        "        valence, arousal, dominance = emotion_dims.chunk(3, dim=-1)\n",
        "\n",
        "        return {\n",
        "            \"token_emotions\": token_emotions,\n",
        "            \"utterance_emotion\": utterance_emotion,\n",
        "            \"conversation_emotions\": conversation_emotions,\n",
        "            \"emotion_dimensions\": {\n",
        "                \"valence\": valence.squeeze(-1),\n",
        "                \"arousal\": arousal.squeeze(-1),\n",
        "                \"dominance\": dominance.squeeze(-1)\n",
        "            }\n",
        "        }"
      ],
      "metadata": {
        "id": "a_arE9HROfDd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emotion Response Generation"
      ],
      "metadata": {
        "id": "rTsHQbZBOlOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionalResponseGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, emotion_dim=8):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emotion_dim = emotion_dim\n",
        "\n",
        "        # Emotion conditioning layer\n",
        "        self.emotion_conditioning = nn.Linear(emotion_dim, d_model)\n",
        "\n",
        "        # Vocabulary biasing\n",
        "        self.emotion_vocab_bias = nn.Linear(emotion_dim, vocab_size)\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, hidden_states, target_emotion):\n",
        "        batch_size, seq_len = hidden_states.size(0), hidden_states.size(1)\n",
        "\n",
        "        # Condition hidden states on target emotion\n",
        "        emotion_features = self.emotion_conditioning(target_emotion).unsqueeze(1)\n",
        "        conditioned_states = hidden_states + emotion_features\n",
        "\n",
        "        # Generate base logits\n",
        "        base_logits = self.output_projection(conditioned_states)\n",
        "\n",
        "        # Apply emotion-specific vocabulary bias\n",
        "        emotion_bias = self.emotion_vocab_bias(target_emotion).unsqueeze(1)\n",
        "        biased_logits = base_logits + 0.1 * emotion_bias  # Scale factor for bias\n",
        "\n",
        "        return biased_logits\n",
        "\n",
        "    def generate_appropriate_emotion(self, user_emotion, context_state):\n",
        "        # Calculate appropriate emotional response\n",
        "        # - Mirror user emotion with dampening for negative emotions\n",
        "        # - Enhance positive emotions slightly\n",
        "        valence = user_emotion[:, 0]  # Assuming first dimension is valence\n",
        "\n",
        "        # If negative emotion, respond with more positive but understanding emotion\n",
        "        response_valence = torch.where(\n",
        "            valence < 0,\n",
        "            valence * 0.5 + 0.2,  # Dampen negative, shift positive\n",
        "            valence * 1.1          # Slightly enhance positive\n",
        "        )\n",
        "\n",
        "        # For other dimensions (arousal, dominance), match with moderation\n",
        "        response_emotion = user_emotion.clone()\n",
        "        response_emotion[:, 0] = response_valence\n",
        "        response_emotion[:, 1] = user_emotion[:, 1] * 0.8  # Moderate arousal\n",
        "\n",
        "        return response_emotion"
      ],
      "metadata": {
        "id": "ZQwrmfxYOrB9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Conversational Initiative System\n",
        " Initiative Detection\n",
        ""
      ],
      "metadata": {
        "id": "JuYIoswROuLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InitiativeSystem(nn.Module):\n",
        "    def __init__(self, d_model, d_hidden=128):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Initiative prediction network\n",
        "        self.initiative_predictor = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Initiative type classifier\n",
        "        self.initiative_type = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden, 4)  # 4 types: question, suggestion, follow-up, info-sharing\n",
        "        )\n",
        "\n",
        "        # Initiative threshold parameters\n",
        "        self.base_threshold = 0.7\n",
        "        self.decay_rate = 0.1\n",
        "\n",
        "    def should_take_initiative(self, context_state, time_since_last_message):\n",
        "        # Calculate initiative score\n",
        "        initiative_score = self.initiative_predictor(context_state).squeeze(-1)\n",
        "\n",
        "        # Calculate adaptive threshold based on time\n",
        "        threshold = self.base_threshold * (1.0 - torch.exp(-self.decay_rate * time_since_last_message))\n",
        "\n",
        "        # Decision to take initiative\n",
        "        should_initiate = initiative_score > threshold\n",
        "\n",
        "        # If initiative should be taken, determine type\n",
        "        initiative_type_logits = self.initiative_type(context_state)\n",
        "        initiative_type = torch.argmax(initiative_type_logits, dim=-1)\n",
        "\n",
        "        return should_initiate, initiative_type\n",
        "\n",
        "    def calculate_follow_up_score(self, project_info):\n",
        "        # Calculate when to follow up on projects/activities\n",
        "        importance = project_info['importance']\n",
        "        time_since_mention = project_info['time_since_last_mentioned']\n",
        "        status = project_info['status_factor']\n",
        "\n",
        "        # Higher score means higher priority for follow-up\n",
        "        follow_up_score = importance * (time_since_mention + 1) * status\n",
        "\n",
        "        return follow_up_score"
      ],
      "metadata": {
        "id": "5ne1xgqbOzOc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project and Activity Tracking"
      ],
      "metadata": {
        "id": "3-SLmLzKO8_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectTracker:\n",
        "    def __init__(self):\n",
        "        self.projects = {}\n",
        "        self.current_time = 0\n",
        "\n",
        "    def update_time(self, increment=1):\n",
        "        self.current_time += increment\n",
        "\n",
        "    def add_project(self, project_id, name, details=\"\", importance=0.5, status=\"active\"):\n",
        "        \"\"\"Add a new project to tracking\"\"\"\n",
        "        self.projects[project_id] = {\n",
        "            \"id\": project_id,\n",
        "            \"name\": name,\n",
        "            \"last_mentioned\": self.current_time,\n",
        "            \"status\": status,\n",
        "            \"details\": details,\n",
        "            \"importance\": importance,\n",
        "            \"follow_up_schedule\": self.current_time + int(10 / importance)  # More important = earlier follow-up\n",
        "        }\n",
        "\n",
        "    def update_project(self, project_id, **kwargs):\n",
        "        \"\"\"Update project information\"\"\"\n",
        "        if project_id in self.projects:\n",
        "            for key, value in kwargs.items():\n",
        "                if key in self.projects[project_id]:\n",
        "                    self.projects[project_id][key] = value\n",
        "\n",
        "            # Auto-update last_mentioned\n",
        "            self.projects[project_id][\"last_mentioned\"] = self.current_time\n",
        "\n",
        "    def get_projects_for_follow_up(self):\n",
        "        \"\"\"Get projects that need follow-up\"\"\"\n",
        "        follow_up_candidates = []\n",
        "\n",
        "        for project_id, project in self.projects.items():\n",
        "            if project[\"status\"] != \"completed\":\n",
        "                # Calculate follow-up score\n",
        "                time_since_mention = self.current_time - project[\"last_mentioned\"]\n",
        "                status_factor = 1.5 if project[\"status\"] == \"blocked\" else 1.0\n",
        "\n",
        "                score = project[\"importance\"] * time_since_mention * status_factor\n",
        "\n",
        "                if score > 5.0:  # Threshold for follow-up\n",
        "                    follow_up_candidates.append((project_id, score))\n",
        "\n",
        "        # Sort by score\n",
        "        follow_up_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return [self.projects[pid] for pid, _ in follow_up_candidates]\n",
        "\n",
        "    def extract_project_info(self, text):\n",
        "        \"\"\"Extract project information from text (simplified)\"\"\"\n",
        "        # In a real implementation, this would use NER and relation extraction\n",
        "        # This is a placeholder implementation\n",
        "\n",
        "        # Simple keyword matching\n",
        "        project_keywords = [\"project\", \"task\", \"work on\", \"assignment\"]\n",
        "\n",
        "        detected_projects = []\n",
        "        # Simplified detection logic\n",
        "        if any(keyword in text.lower() for keyword in project_keywords):\n",
        "            # Extract project name (simplified)\n",
        "            words = text.split()\n",
        "            for i, word in enumerate(words):\n",
        "                if word.lower() in project_keywords and i+1 < len(words):\n",
        "                    project_name = words[i+1]\n",
        "\n",
        "                    # Generate ID\n",
        "                    project_id = f\"proj_{len(self.projects)}\"\n",
        "\n",
        "                    # Estimate importance (simplified)\n",
        "                    importance_words = [\"urgent\", \"important\", \"critical\", \"crucial\"]\n",
        "                    importance = 0.5  # Default\n",
        "                    for imp_word in importance_words:\n",
        "                        if imp_word in text.lower():\n",
        "                            importance = 0.8\n",
        "                            break\n",
        "\n",
        "                    detected_projects.append({\n",
        "                        \"id\": project_id,\n",
        "                        \"name\": project_name,\n",
        "                        \"importance\": importance\n",
        "                    })\n",
        "\n",
        "        return detected_projects"
      ],
      "metadata": {
        "id": "Lis_oBEPPAw5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integration Architecture"
      ],
      "metadata": {
        "id": "V0P0TrSVPK0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextAwareEmotionalLLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        d_model=768,\n",
        "        n_layers=12,\n",
        "        n_heads=12,\n",
        "        max_seq_len=2048,\n",
        "        emotion_dim=8\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layers\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = RelativePositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        # Transformer layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model, n_heads) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Memory system\n",
        "        self.memory = HierarchicalMemoryBlock(d_model)\n",
        "\n",
        "        # Emotion system\n",
        "        self.emotion_recognition = EmotionRecognitionModule(d_model, emotion_dim)\n",
        "        self.emotional_generator = EmotionalResponseGenerator(vocab_size, d_model, emotion_dim)\n",
        "\n",
        "        # Initiative system\n",
        "        self.initiative_system = InitiativeSystem(d_model)\n",
        "\n",
        "        # Project tracking\n",
        "        self.project_tracker = ProjectTracker()\n",
        "\n",
        "        # Output layer\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, context_ids=None):\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        # Embeddings\n",
        "        token_embeds = self.token_embedding(input_ids)\n",
        "        position_embeds = self.positional_encoding(token_embeds)\n",
        "        hidden_states = token_embeds + position_embeds\n",
        "\n",
        "        # Process through transformer layers\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states, attention_mask)\n",
        "\n",
        "        # Retrieve from memory if context provided\n",
        "        if context_ids is not None:\n",
        "            context_embeds = self.token_embedding(context_ids)\n",
        "            memory_states = self.memory.retrieve_memory(context_embeds)\n",
        "            hidden_states = hidden_states + memory_states\n",
        "\n",
        "        # Recognize emotions\n",
        "        emotion_data = self.emotion_recognition(hidden_states)\n",
        "        user_emotion = emotion_data[\"utterance_emotion\"]\n",
        "\n",
        "        # Generate appropriate emotional response\n",
        "        target_emotion = self.emotional_generator.generate_appropriate_emotion(\n",
        "            user_emotion, hidden_states[:, -1]\n",
        "        )\n",
        "\n",
        "        # Check for initiative\n",
        "        time_since_last = torch.ones(batch_size)  # Placeholder, would come from context\n",
        "        should_initiate, initiative_type = self.initiative_system.should_take_initiative(\n",
        "            hidden_states[:, -1], time_since_last\n",
        "        )\n",
        "\n",
        "        # Generate output logits with emotional conditioning\n",
        "        logits = self.emotional_generator(hidden_states, target_emotion)\n",
        "\n",
        "        # Store important information in memory\n",
        "        self.memory.store_memory(hidden_states[:, -1].detach(), \"active\")\n",
        "\n",
        "        # Update project tracking from input\n",
        "        for b in range(batch_size):\n",
        "            text = \"Sample text\"  # Would decode from input_ids\n",
        "            projects = self.project_tracker.extract_project_info(text)\n",
        "            for project in projects:\n",
        "                self.project_tracker.add_project(**project)\n",
        "\n",
        "        return {\n",
        "            \"logits\": logits,\n",
        "            \"emotion_data\": emotion_data,\n",
        "            \"target_emotion\": target_emotion,\n",
        "            \"initiative\": {\n",
        "                \"should_initiate\": should_initiate,\n",
        "                \"initiative_type\": initiative_type\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.7, do_sample=True):\n",
        "        \"\"\"Simple generation function\"\"\"\n",
        "        batch_size = input_ids.size(0)\n",
        "        current_ids = input_ids.clone()\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass\n",
        "            outputs = self.forward(current_ids)\n",
        "            next_token_logits = outputs[\"logits\"][:, -1, :]\n",
        "\n",
        "            # Apply temperature\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "\n",
        "            # Sample or greedy\n",
        "            if do_sample:\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "            # Concatenate new tokens\n",
        "            current_ids = torch.cat([current_ids, next_token], dim=1)\n",
        "\n",
        "            # Check for EOS\n",
        "            if (next_token == eos_token_id).all():\n",
        "                break\n",
        "\n",
        "        return current_ids"
      ],
      "metadata": {
        "id": "U22iq-WkPLwi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Optimization Framework\n",
        "\n",
        "Multi-Objective Training Strategy"
      ],
      "metadata": {
        "id": "beB9MdB1PQaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_framework(model, train_dataloader, val_dataloader, num_epochs=10):\n",
        "    # Optimizer setup\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    total_steps = len(train_dataloader) * num_epochs\n",
        "    warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
        "\n",
        "        # Validate\n",
        "        val_loss = validate_model(model, val_dataloader, device)\n",
        "\n",
        "        # Track metrics\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint if improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_checkpoint(model, optimizer, scheduler, epoch, val_loss)\n",
        "\n",
        "    # Final evaluation\n",
        "    test_metrics = evaluate_model(model, test_dataloader, device)\n",
        "\n",
        "    return model, test_metrics"
      ],
      "metadata": {
        "id": "W_z7IjjAPYgJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preference Learning and Reinforcement"
      ],
      "metadata": {
        "id": "7MZ1JQVWPbAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModel(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.reward_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # Extract CLS token or use mean pooling\n",
        "        pooled = hidden_states.mean(dim=1)\n",
        "        reward = self.reward_head(pooled)\n",
        "        return reward\n",
        "\n",
        "def train_with_dpo(model, optimizer, dataloader, reward_model, beta=0.1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Get preferred and rejected responses for each context\n",
        "        contexts = batch[\"contexts\"].to(device)\n",
        "        preferred = batch[\"preferred_responses\"].to(device)\n",
        "        rejected = batch[\"rejected_responses\"].to(device)\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.no_grad():\n",
        "            # Generate log probs for current model\n",
        "            preferred_outputs = model(contexts, preferred)\n",
        "            rejected_outputs = model(contexts, rejected)\n",
        "\n",
        "            # Get reward scores from reward model\n",
        "            preferred_rewards = reward_model(preferred_outputs.hidden_states).squeeze(-1)\n",
        "            rejected_rewards = reward_model(rejected_outputs.hidden_states).squeeze(-1)\n",
        "\n",
        "        # Calculate DPO loss\n",
        "        # L_DPO(θ) = -E[(log σ(β(r_θ(x,y_w) - r_θ(x,y_l)))]\n",
        "        reward_diff = preferred_rewards - rejected_rewards\n",
        "        loss = -torch.log(torch.sigmoid(beta * reward_diff)).mean()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "qzR7p5E5PhM8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continual Learning Framework"
      ],
      "metadata": {
        "id": "4hkWgUmiPnqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset, Subset\n",
        "import copy\n",
        "from typing import Dict, List, Tuple, Optional, Union, Callable, Any\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ContinualLearningFramework:\n",
        "    \"\"\"\n",
        "    A comprehensive framework for implementing various continual learning methods.\n",
        "    This framework supports:\n",
        "    - Experience Replay\n",
        "    - Elastic Weight Consolidation (EWC)\n",
        "    - Knowledge Distillation\n",
        "    - Task-specific adapters\n",
        "    - Learning without Forgetting (LwF)\n",
        "    - Synaptic Intelligence (SI)\n",
        "    - Functional Regularization\n",
        "    - Progressive Neural Networks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer_class: torch.optim.Optimizer = optim.Adam,\n",
        "        optimizer_kwargs: Dict = {'lr': 0.001},\n",
        "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "        # Experience Replay parameters\n",
        "        replay_buffer_size: int = 1000,\n",
        "        replay_strategy: str = 'random',  # 'random', 'prioritized', 'class_balanced'\n",
        "        replay_frequency: int = 5,  # Every N batches\n",
        "\n",
        "        # EWC parameters\n",
        "        use_ewc: bool = False,\n",
        "        ewc_lambda: float = 100.0,\n",
        "        ewc_fisher_sample_size: int = 200,\n",
        "        ewc_mode: str = 'separate',  # 'separate' or 'online'\n",
        "\n",
        "        # Distillation parameters\n",
        "        use_distillation: bool = False,\n",
        "        distillation_temp: float = 2.0,\n",
        "        distillation_alpha: float = 0.5,\n",
        "\n",
        "        # Adapter parameters\n",
        "        use_adapters: bool = False,\n",
        "        adapter_size: int = 64,\n",
        "        freeze_base_model: bool = True,\n",
        "\n",
        "        # LwF parameters\n",
        "        use_lwf: bool = False,\n",
        "        lwf_alpha: float = 1.0,\n",
        "        lwf_temp: float = 2.0,\n",
        "\n",
        "        # Synaptic Intelligence parameters\n",
        "        use_si: bool = False,\n",
        "        si_lambda: float = 1.0,\n",
        "        si_omega_decay: float = 0.95,\n",
        "\n",
        "        # Progressive networks parameters\n",
        "        use_progressive_nets: bool = False,\n",
        "\n",
        "        # Functional regularization parameters\n",
        "        use_func_regularization: bool = False,\n",
        "        func_reg_lambda: float = 1.0,\n",
        "        func_sample_size: int = 200,\n",
        "\n",
        "        # Misc parameters\n",
        "        checkpoint_dir: str = './checkpoints',\n",
        "        log_metrics: bool = True,\n",
        "        verbose: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the continual learning framework.\n",
        "\n",
        "        Args:\n",
        "            model: The neural network model\n",
        "            optimizer_class: The optimizer class to use\n",
        "            optimizer_kwargs: Arguments for the optimizer\n",
        "            device: Device to run the model on\n",
        "\n",
        "            # Experience Replay parameters\n",
        "            replay_buffer_size: Size of experience replay buffer\n",
        "            replay_strategy: Strategy for sampling from replay buffer ('random', 'prioritized', 'class_balanced')\n",
        "            replay_frequency: How often to perform replay (every N batches)\n",
        "\n",
        "            # EWC parameters\n",
        "            use_ewc: Whether to use Elastic Weight Consolidation\n",
        "            ewc_lambda: EWC regularization strength\n",
        "            ewc_fisher_sample_size: Number of samples to estimate Fisher information\n",
        "            ewc_mode: 'separate' (maintain separate Fisher matrices) or 'online' (update single matrix)\n",
        "\n",
        "            # Distillation parameters\n",
        "            use_distillation: Whether to use knowledge distillation\n",
        "            distillation_temp: Temperature for distillation\n",
        "            distillation_alpha: Weight for distillation loss\n",
        "\n",
        "            # Adapter parameters\n",
        "            use_adapters: Whether to use task-specific adapters\n",
        "            adapter_size: Size of adapter hidden representations\n",
        "            freeze_base_model: Whether to freeze the base model when using adapters\n",
        "\n",
        "            # LwF parameters\n",
        "            use_lwf: Whether to use Learning without Forgetting\n",
        "            lwf_alpha: Weight for LwF loss\n",
        "            lwf_temp: Temperature for LwF\n",
        "\n",
        "            # Synaptic Intelligence parameters\n",
        "            use_si: Whether to use Synaptic Intelligence\n",
        "            si_lambda: SI regularization strength\n",
        "            si_omega_decay: Decay rate for importance weights\n",
        "\n",
        "            # Progressive networks parameters\n",
        "            use_progressive_nets: Whether to use Progressive Neural Networks\n",
        "\n",
        "            # Functional regularization parameters\n",
        "            use_func_regularization: Whether to use functional regularization\n",
        "            func_reg_lambda: Weight for functional regularization\n",
        "            func_sample_size: Number of samples for functional regularization\n",
        "\n",
        "            # Misc parameters\n",
        "            checkpoint_dir: Directory to save checkpoints\n",
        "            log_metrics: Whether to log metrics\n",
        "            verbose: Whether to print verbose output\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.base_model = copy.deepcopy(model) if use_progressive_nets else None\n",
        "        self.optimizer_class = optimizer_class\n",
        "        self.optimizer_kwargs = optimizer_kwargs\n",
        "        self.device = device\n",
        "        self.current_task_id = None\n",
        "        self.seen_tasks = set()\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Experience Replay\n",
        "        self.replay_buffer_size = replay_buffer_size\n",
        "        self.replay_strategy = replay_strategy\n",
        "        self.replay_frequency = replay_frequency\n",
        "        self.replay_buffer = {}  # task_id -> [(x, y, importance), ...]\n",
        "        self.replay_counter = 0\n",
        "\n",
        "        # EWC\n",
        "        self.use_ewc = use_ewc\n",
        "        self.ewc_lambda = ewc_lambda\n",
        "        self.ewc_fisher_sample_size = ewc_fisher_sample_size\n",
        "        self.ewc_mode = ewc_mode\n",
        "        self.fisher_dict = {}  # task_id -> {param_name: fisher}\n",
        "        self.param_dict = {}   # task_id -> {param_name: param_value}\n",
        "\n",
        "        # Knowledge Distillation\n",
        "        self.use_distillation = use_distillation\n",
        "        self.distillation_temp = distillation_temp\n",
        "        self.distillation_alpha = distillation_alpha\n",
        "        self.teacher_model = None\n",
        "\n",
        "        # Task-specific Adapters\n",
        "        self.use_adapters = use_adapters\n",
        "        self.adapter_size = adapter_size\n",
        "        self.freeze_base_model = freeze_base_model\n",
        "        self.adapters = nn.ModuleDict()  # task_id -> adapter\n",
        "\n",
        "        # Learning without Forgetting\n",
        "        self.use_lwf = use_lwf\n",
        "        self.lwf_alpha = lwf_alpha\n",
        "        self.lwf_temp = lwf_temp\n",
        "        self.lwf_models = {}  # task_id -> model\n",
        "\n",
        "        # Synaptic Intelligence\n",
        "        self.use_si = use_si\n",
        "        self.si_lambda = si_lambda\n",
        "        self.si_omega_decay = si_omega_decay\n",
        "        self.si_omega = {}  # Parameter importance\n",
        "        self.si_prev_params = {}  # Previous parameter values\n",
        "        self.si_accumulated_delta = {}  # Accumulated delta*gradient\n",
        "\n",
        "        # Progressive Neural Networks\n",
        "        self.use_progressive_nets = use_progressive_nets\n",
        "        self.column_models = nn.ModuleDict()  # task_id -> column model\n",
        "\n",
        "        # Functional Regularization\n",
        "        self.use_func_regularization = use_func_regularization\n",
        "        self.func_reg_lambda = func_reg_lambda\n",
        "        self.func_sample_size = func_sample_size\n",
        "        self.func_samples = {}  # task_id -> [(x, output), ...]\n",
        "\n",
        "        # Metrics and Logging\n",
        "        self.log_metrics = log_metrics\n",
        "        self.metrics = defaultdict(list)\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = self.optimizer_class(self.model.parameters(), **self.optimizer_kwargs)\n",
        "\n",
        "        # Initialize SI parameters if needed\n",
        "        if self.use_si:\n",
        "            self._init_si_params()\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Initialized Continual Learning Framework on device: {device}\")\n",
        "            logger.info(f\"Active methods: EWC={use_ewc}, Distillation={use_distillation}, Adapters={use_adapters}, \"\n",
        "                      f\"LwF={use_lwf}, SI={use_si}, ProgressiveNets={use_progressive_nets}, \"\n",
        "                      f\"FunctionalReg={use_func_regularization}\")\n",
        "\n",
        "    def _init_si_params(self):\n",
        "        \"\"\"Initialize parameters for Synaptic Intelligence.\"\"\"\n",
        "        for n, p in self.model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.si_omega[n] = torch.zeros_like(p.data)\n",
        "                self.si_prev_params[n] = p.data.clone()\n",
        "                self.si_accumulated_delta[n] = torch.zeros_like(p.data)\n",
        "\n",
        "    def _compute_fisher_information(self, data_loader: DataLoader, num_samples: int = None) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Compute the Fisher information matrix for parameters.\n",
        "\n",
        "        Args:\n",
        "            data_loader: DataLoader with the current task data\n",
        "            num_samples: Number of samples to use for estimation\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with parameter names and their Fisher information\n",
        "        \"\"\"\n",
        "        if num_samples is None:\n",
        "            num_samples = self.ewc_fisher_sample_size\n",
        "\n",
        "        self.model.eval()\n",
        "        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Computing Fisher information matrix using {num_samples} samples\")\n",
        "\n",
        "        sample_count = 0\n",
        "        for data_batch in data_loader:\n",
        "            if sample_count >= num_samples:\n",
        "                break\n",
        "\n",
        "            inputs, targets = data_batch\n",
        "            inputs = inputs.to(self.device)\n",
        "            targets = targets.to(self.device)\n",
        "            batch_size = inputs.shape[0]\n",
        "\n",
        "            self.model.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "\n",
        "            # For each sample, compute the fisher information\n",
        "            for i in range(min(batch_size, num_samples - sample_count)):\n",
        "                log_prob = torch.nn.functional.log_softmax(outputs[i:i+1], dim=1)\n",
        "\n",
        "                # Use actual prediction for fisher computation\n",
        "                pred = torch.argmax(log_prob, dim=1)\n",
        "                loss = -log_prob[0, pred]\n",
        "                loss.backward(retain_graph=(i < batch_size - 1))\n",
        "\n",
        "                # Accumulate the gradients\n",
        "                for n, p in self.model.named_parameters():\n",
        "                    if p.grad is not None and p.requires_grad:\n",
        "                        fisher[n] += p.grad.pow(2).detach() / num_samples\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                sample_count += 1\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Fisher information matrix computed with {sample_count} samples\")\n",
        "        return fisher\n",
        "\n",
        "    def _save_current_parameters(self) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Save the current parameters of the model.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with parameter names and their values\n",
        "        \"\"\"\n",
        "        return {n: p.detach().clone() for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "\n",
        "    def _add_to_replay_buffer(self, task_id: str, data: List[Tuple[torch.Tensor, torch.Tensor]], compute_importance: bool = False):\n",
        "        \"\"\"\n",
        "        Add data to the replay buffer for a given task.\n",
        "\n",
        "        Args:\n",
        "            task_id: Identifier for the task\n",
        "            data: List of (input, target) tuples\n",
        "            compute_importance: Whether to compute importance scores for prioritized replay\n",
        "        \"\"\"\n",
        "        if task_id not in self.replay_buffer:\n",
        "            self.replay_buffer[task_id] = []\n",
        "\n",
        "        # Compute importance scores if using prioritized replay\n",
        "        if compute_importance and self.replay_strategy == 'prioritized':\n",
        "            with torch.no_grad():\n",
        "                for x, y in data:\n",
        "                    x = x.to(self.device).unsqueeze(0)\n",
        "                    y = y.to(self.device).unsqueeze(0)\n",
        "                    output = self.model(x)\n",
        "                    prob = torch.softmax(output, dim=1)\n",
        "                    # Importance is inverse of confidence (lower confidence -> higher importance)\n",
        "                    importance = 1.0 - prob[0, y.item()].item()\n",
        "                    self.replay_buffer[task_id].append((x.squeeze(0).cpu(), y.squeeze(0).cpu(), importance))\n",
        "        else:\n",
        "            # Default importance of 0.5 for random or class-balanced replay\n",
        "            self.replay_buffer[task_id].extend([(x, y, 0.5) for x, y in data])\n",
        "\n",
        "        # Limit the buffer size by sampling according to the chosen strategy\n",
        "        if len(self.replay_buffer[task_id]) > self.replay_buffer_size:\n",
        "            if self.replay_strategy == 'random':\n",
        "                indices = np.random.choice(\n",
        "                    len(self.replay_buffer[task_id]),\n",
        "                    self.replay_buffer_size,\n",
        "                    replace=False\n",
        "                )\n",
        "                self.replay_buffer[task_id] = [self.replay_buffer[task_id][i] for i in indices]\n",
        "\n",
        "            elif self.replay_strategy == 'prioritized':\n",
        "                # Sort by importance (highest first) and keep top samples\n",
        "                self.replay_buffer[task_id].sort(key=lambda x: x[2], reverse=True)\n",
        "                self.replay_buffer[task_id] = self.replay_buffer[task_id][:self.replay_buffer_size]\n",
        "\n",
        "            elif self.replay_strategy == 'class_balanced':\n",
        "                # Group by class and sample equally from each class\n",
        "                class_samples = {}\n",
        "                for x, y, imp in self.replay_buffer[task_id]:\n",
        "                    y_item = y.item()\n",
        "                    if y_item not in class_samples:\n",
        "                        class_samples[y_item] = []\n",
        "                    class_samples[y_item].append((x, y, imp))\n",
        "\n",
        "                # Calculate samples per class\n",
        "                num_classes = len(class_samples)\n",
        "                samples_per_class = self.replay_buffer_size // num_classes\n",
        "                remainder = self.replay_buffer_size % num_classes\n",
        "\n",
        "                # Create a balanced buffer\n",
        "                balanced_buffer = []\n",
        "                for cls, samples in class_samples.items():\n",
        "                    # Add extra sample to classes with remainder\n",
        "                    cls_samples = samples_per_class + (1 if remainder > 0 else 0)\n",
        "                    remainder -= 1 if remainder > 0 else 0\n",
        "\n",
        "                    # Random sample if we have more than needed\n",
        "                    if len(samples) > cls_samples:\n",
        "                        indices = np.random.choice(len(samples), cls_samples, replace=False)\n",
        "                        balanced_buffer.extend([samples[i] for i in indices])\n",
        "                    else:\n",
        "                        balanced_buffer.extend(samples)\n",
        "\n",
        "                self.replay_buffer[task_id] = balanced_buffer\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Added data to replay buffer for task {task_id}, buffer size: {len(self.replay_buffer[task_id])}\")\n",
        "\n",
        "    def _create_adapter(self, task_id: str, input_dim: int, output_dim: int):\n",
        "        \"\"\"\n",
        "        Create a task-specific adapter module.\n",
        "\n",
        "        Args:\n",
        "            task_id: Task identifier\n",
        "            input_dim: Input dimension of the adapter\n",
        "            output_dim: Output dimension of the adapter\n",
        "        \"\"\"\n",
        "        adapter = nn.Sequential(\n",
        "            nn.Linear(input_dim, self.adapter_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.adapter_size, output_dim)\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.adapters[task_id] = adapter\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Created adapter for task {task_id}\")\n",
        "\n",
        "    def _create_column_model(self, task_id: str):\n",
        "        \"\"\"\n",
        "        Create a new column model for Progressive Neural Networks.\n",
        "\n",
        "        Args:\n",
        "            task_id: Task identifier\n",
        "        \"\"\"\n",
        "        # Create a copy of the base model\n",
        "        column_model = copy.deepcopy(self.base_model).to(self.device)\n",
        "\n",
        "        # Create lateral connections to previous columns\n",
        "        # This is a simplified implementation; a real implementation would need\n",
        "        # specific lateral connection layers between columns\n",
        "\n",
        "        self.column_models[task_id] = column_model\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Created column model for task {task_id}\")\n",
        "\n",
        "    def _update_si_weights(self):\n",
        "        \"\"\"Update the importance weights for Synaptic Intelligence.\"\"\"\n",
        "        for n, p in self.model.named_parameters():\n",
        "            if n in self.si_accumulated_delta and p.requires_grad:\n",
        "                # Calculate change in parameter\n",
        "                delta = p.data - self.si_prev_params[n]\n",
        "\n",
        "                # Update omega (importance) based on accumulated gradients\n",
        "                if torch.norm(delta) > 0:\n",
        "                    self.si_omega[n] += self.si_accumulated_delta[n] / (delta ** 2 + 1e-7)\n",
        "\n",
        "                # Apply decay to previous omega values\n",
        "                self.si_omega[n] *= self.si_omega_decay\n",
        "\n",
        "                # Reset accumulated delta and update previous parameters\n",
        "                self.si_accumulated_delta[n] = torch.zeros_like(p.data)\n",
        "                self.si_prev_params[n] = p.data.clone()\n",
        "\n",
        "    def _collect_functional_samples(self, data_loader: DataLoader, task_id: str, num_samples: int = None):\n",
        "        \"\"\"\n",
        "        Collect input-output pairs for functional regularization.\n",
        "\n",
        "        Args:\n",
        "            data_loader: DataLoader with task data\n",
        "            task_id: Task identifier\n",
        "            num_samples: Number of samples to collect\n",
        "        \"\"\"\n",
        "        if num_samples is None:\n",
        "            num_samples = self.func_sample_size\n",
        "\n",
        "        if task_id not in self.func_samples:\n",
        "            self.func_samples[task_id] = []\n",
        "\n",
        "        self.model.eval()\n",
        "        sample_count = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data_batch in data_loader:\n",
        "                if sample_count >= num_samples:\n",
        "                    break\n",
        "\n",
        "                inputs, _ = data_batch\n",
        "                inputs = inputs.to(self.device)\n",
        "                batch_size = inputs.shape[0]\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "\n",
        "                for i in range(min(batch_size, num_samples - sample_count)):\n",
        "                    self.func_samples[task_id].append((\n",
        "                        inputs[i].detach().cpu(),\n",
        "                        outputs[i].detach().cpu()\n",
        "                    ))\n",
        "                    sample_count += 1\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Collected {sample_count} functional samples for task {task_id}\")\n",
        "\n",
        "    def start_task(self, task_id: str):\n",
        "        \"\"\"\n",
        "        Start training on a new task.\n",
        "\n",
        "        Args:\n",
        "            task_id: Identifier for the task\n",
        "        \"\"\"\n",
        "        self.current_task_id = task_id\n",
        "\n",
        "        # For Progressive Neural Networks\n",
        "        if self.use_progressive_nets:\n",
        "            if task_id not in self.column_models:\n",
        "                self._create_column_model(task_id)\n",
        "\n",
        "        # For task adapters\n",
        "        if self.use_adapters:\n",
        "            if task_id in self.adapters:\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"Switching to existing adapter for task {task_id}\")\n",
        "\n",
        "                # Freeze or unfreeze parameters based on mode\n",
        "                if self.freeze_base_model:\n",
        "                    for param in self.model.parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "                    # Only unfreeze the adapter parameters\n",
        "                    for param in self.adapters[task_id].parameters():\n",
        "                        param.requires_grad = True\n",
        "\n",
        "        # Save teacher model for distillation if needed\n",
        "        if (self.use_distillation or self.use_lwf) and task_id not in self.seen_tasks:\n",
        "            if self.verbose:\n",
        "                logger.info(\"Saving teacher model for knowledge distillation/LwF\")\n",
        "            self.teacher_model = copy.deepcopy(self.model)\n",
        "            self.teacher_model.eval()\n",
        "\n",
        "            if self.use_lwf:\n",
        "                self.lwf_models[task_id] = self.teacher_model\n",
        "\n",
        "        # Create a new optimizer for the task\n",
        "        if self.use_adapters and task_id in self.adapters and self.freeze_base_model:\n",
        "            # Only optimize the adapter parameters for existing tasks\n",
        "            self.optimizer = self.optimizer_class(\n",
        "                self.adapters[task_id].parameters(),\n",
        "                **self.optimizer_kwargs\n",
        "            )\n",
        "        elif self.use_progressive_nets:\n",
        "            # Only optimize the current column parameters\n",
        "            self.optimizer = self.optimizer_class(\n",
        "                self.column_models[task_id].parameters(),\n",
        "                **self.optimizer_kwargs\n",
        "            )\n",
        "        else:\n",
        "            # Optimize all parameters for new tasks\n",
        "            self.optimizer = self.optimizer_class(\n",
        "                self.model.parameters(),\n",
        "                **self.optimizer_kwargs\n",
        "            )\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Started task {task_id}\")\n",
        "\n",
        "    def _accumulate_si_gradients(self):\n",
        "        \"\"\"Accumulate gradients for Synaptic Intelligence.\"\"\"\n",
        "        for n, p in self.model.named_parameters():\n",
        "            if n in self.si_accumulated_delta and p.grad is not None and p.requires_grad:\n",
        "                # Accumulate gradient * (current param - initial param)\n",
        "                self.si_accumulated_delta[n] -= p.grad * (p.data - self.si_prev_params[n])\n",
        "\n",
        "    def _compute_si_loss(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the Synaptic Intelligence regularization loss.\n",
        "\n",
        "        Returns:\n",
        "            SI loss\n",
        "        \"\"\"\n",
        "        si_loss = 0\n",
        "        for n, p in self.model.named_parameters():\n",
        "            if n in self.si_omega and n in self.si_prev_params and p.requires_grad:\n",
        "                # Compute quadratic penalty on parameter changes\n",
        "                si_loss += (self.si_omega[n] * (p - self.si_prev_params[n]) ** 2).sum()\n",
        "        return si_loss\n",
        "\n",
        "    def _compute_functional_regularization_loss(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the functional regularization loss.\n",
        "\n",
        "        Returns:\n",
        "            Functional regularization loss\n",
        "        \"\"\"\n",
        "        func_loss = 0\n",
        "        sample_count = 0\n",
        "\n",
        "        # Sample from previous tasks\n",
        "        for task_id, samples in self.func_samples.items():\n",
        "            if task_id == self.current_task_id:\n",
        "                continue\n",
        "\n",
        "            # Randomly sample from the stored functional samples\n",
        "            indices = np.random.choice(len(samples), min(len(samples), 20), replace=False)\n",
        "\n",
        "            for idx in indices:\n",
        "                x, y_prev = samples[idx]\n",
        "                x = x.to(self.device).unsqueeze(0)\n",
        "                y_prev = y_prev.to(self.device)\n",
        "\n",
        "                # Forward pass with current model\n",
        "                y_current = self.model(x).squeeze(0)\n",
        "\n",
        "                # Mean squared error between previous and current outputs\n",
        "                func_loss += torch.mean((y_current - y_prev) ** 2)\n",
        "                sample_count += 1\n",
        "\n",
        "        return func_loss / max(1, sample_count)\n",
        "\n",
        "    def train_step(\n",
        "        self,\n",
        "        inputs: torch.Tensor,\n",
        "        targets: torch.Tensor,\n",
        "        task_specific_loss_fn: Callable = nn.CrossEntropyLoss()\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Perform a single training step.\n",
        "\n",
        "        Args:\n",
        "            inputs: Input batch\n",
        "            targets: Target batch\n",
        "            task_specific_loss_fn: Loss function specific to the current task\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with loss metrics\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        # Move data to device\n",
        "        inputs = inputs.to(self.device)\n",
        "        targets = targets.to(self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "\n",
        "        # Task-specific loss\n",
        "        task_loss = task_specific_loss_fn(outputs, targets)\n",
        "        total_loss = task_loss\n",
        "        loss_metrics = {\"task_loss\": task_loss.item()}\n",
        "\n",
        "        # Knowledge distillation loss\n",
        "        if self.use_distillation and self.teacher_model is not None:\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = self.teacher_model(inputs)\n",
        "\n",
        "            # Compute distillation loss\n",
        "            distillation_loss = self._compute_distillation_loss(\n",
        "                outputs, teacher_outputs, self.distillation_temp\n",
        "            )\n",
        "\n",
        "            # Combine losses\n",
        "            total_loss = (\n",
        "                (1 - self.distillation_alpha) * task_loss +\n",
        "                self.distillation_alpha * distillation_loss\n",
        "            )\n",
        "            loss_metrics[\"distillation_loss\"] = distillation_loss.item()\n",
        "\n",
        "        # Learning without Forgetting loss\n",
        "        if self.use_lwf and self.lwf_models:\n",
        "            lwf_loss = 0\n",
        "            for task_id, old_model in self.lwf_models.items():\n",
        "                if task_id == self.current_task_id:\n",
        "                    continue\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    old_outputs = old_model(inputs)\n",
        "\n",
        "                # Compute LwF loss\n",
        "                lwf_task_loss = self._compute_distillation_loss(\n",
        "                    outputs, old_outputs, self.lwf_temp\n",
        "                )\n",
        "                lwf_loss += lwf_task_loss\n",
        "\n",
        "            if lwf_loss > 0:\n",
        "                total_loss += self.lwf_alpha * lwf_loss\n",
        "                loss_metrics[\"lwf_loss\"] = lwf_loss.item()\n",
        "\n",
        "        # EWC loss\n",
        "        if self.use_ewc and self.fisher_dict and self.param_dict:\n",
        "            ewc_loss = self._compute_ewc_loss()\n",
        "            total_loss += self.ewc_lambda * ewc_loss\n",
        "            loss_metrics[\"ewc_loss\"] = ewc_loss.item()\n",
        "\n",
        "        # Synaptic Intelligence loss\n",
        "        if self.use_si and self.si_omega:\n",
        "            si_loss = self._compute_si_loss()\n",
        "            total_loss += self.si_lambda * si_loss\n",
        "            loss_metrics[\"si_loss\"] = si_loss.item()\n",
        "\n",
        "        # Functional regularization loss\n",
        "        if self.use_func_regularization and self.func_samples:\n",
        "            func_loss = self._compute_functional_regularization_loss()\n",
        "            total_loss += self.func_reg_lambda * func_loss\n",
        "            loss_metrics[\"func_reg_loss\"] = func_loss.item()\n",
        "\n",
        "        # Backward pass and optimizer step\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Accumulate gradients for Synaptic Intelligence\n",
        "        if self.use_si:\n",
        "            self._accumulate_si_gradients()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Increment the replay counter\n",
        "        self.replay_counter += 1\n",
        "\n",
        "        loss_metrics[\"total_loss\"] = total_loss.item()\n",
        "        return loss_metrics\n",
        "\n",
        "    def _compute_distillation_loss(\n",
        "        self,\n",
        "        outputs: torch.Tensor,\n",
        "        teacher_outputs: torch.Tensor,\n",
        "        temperature: float\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the knowledge distillation loss.\n",
        "\n",
        "        Args:\n",
        "            outputs: Model outputs\n",
        "            teacher_outputs: Teacher model outputs\n",
        "            temperature: Softmax temperature\n",
        "\n",
        "        Returns:\n",
        "            Distillation loss\n",
        "        \"\"\"\n",
        "        soft_targets = torch.nn.functional.softmax(teacher_outputs / temperature, dim=1)\n",
        "        log_probs = torch.nn.functional.log_softmax(outputs / temperature, dim=1)\n",
        "        distillation_loss = -torch.sum(soft_targets * log_probs) / outputs.size(0)\n",
        "        return distillation_loss * (temperature ** 2)\n",
        "\n",
        "    def _compute_ewc_loss(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the EWC regularization loss.\n",
        "\n",
        "        Returns:\n",
        "            EWC loss\n",
        "        \"\"\"\n",
        "        ewc_loss = 0\n",
        "\n",
        "        if self.ewc_mode == 'separate':\n",
        "            # Use separate Fisher matrix for each task\n",
        "            for task_id in self.seen_tasks:\n",
        "                if task_id in self.fisher_dict and task_id in self.param_dict:\n",
        "                    for n, p in self.model.named_parameters():\n",
        "                        if n in self.fisher_dict[task_id] and n in self.param_dict[task_id] and p.requires_grad:\n",
        "                            fisher = self.fisher_dict[task_id][n]\n",
        "                            old_param = self.param_dict[task_id][n]\n",
        "                            ewc_loss += torch.sum(fisher * (p - old_param) ** 2)\n",
        "        elif self.ewc_mode == 'online':\n",
        "            # Use a single Fisher matrix that's updated online\n",
        "            if 'online' in self.fisher_dict and 'online' in self.param_dict:\n",
        "                for n, p in self.model.named_parameters():\n",
        "                    if n in self.fisher_dict['online'] and n in self.param_dict['online'] and p.requires_grad:\n",
        "                        fisher = self.fisher_dict['online'][n]\n",
        "                        old_param = self.param_dict['online'][n]\n",
        "                        ewc_loss += torch.sum(fisher * (p - old_param) ** 2)\n",
        "\n",
        "        return ewc_loss\n",
        "\n",
        "    def _sample_from_replay_buffer(self, batch_size: int = 32) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Sample a batch from the replay buffer.\n",
        "\n",
        "        Args:\n",
        "            batch_size: Number of samples to draw\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (inputs, targets)\n",
        "        \"\"\"\n",
        "        all_experiences = []\n",
        "        for task_id, experiences in self.replay_buffer.items():\n",
        "            if task_id != self.current_task_id:  # Don't replay current task\n",
        "                all_experiences.extend(experiences)\n",
        "\n",
        "        if not all_experiences:\n",
        "            return None, None\n",
        "\n",
        "        # Sample according to strategy\n",
        "        if self.replay_strategy == 'random':\n",
        "            indices = np.random.choice(len(all_experiences), min(len(all_experiences), batch_size), replace=False)\n",
        "            sampled_experiences = [all_experiences[i] for i in indices]\n",
        "\n",
        "        elif self.replay_strategy == 'prioritized':\n",
        "            # Sample based on importance (higher importance = higher probability)\n",
        "            importance = np.array([exp[2] for exp in all_experiences])\n",
        "            probs = importance / importance.sum()\n",
        "            indices = np.random.choice(\n",
        "                len(all_experiences),\n",
        "                min(len(all_experiences), batch_size),\n",
        "                replace=False,\n",
        "                p=probs\n",
        "            )\n",
        "            sampled_experiences = [all_experiences[i] for i in indices]\n",
        "\n",
        "        elif self.replay_strategy == 'class_balanced':\n",
        "            # Group by class\n",
        "            class_samples = {}\n",
        "            for x, y, imp in all_experiences:\n",
        "                y_item = y.item()\n",
        "                if y_item not in class_"
      ],
      "metadata": {
        "id": "fsmd0r6UQx6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}