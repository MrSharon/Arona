{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOb7ql21bZMPJcAJ9n11CBg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSharon/Arona/blob/main/Arona.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Core Model Architecture"
      ],
      "metadata": {
        "id": "EKPwUrUwJs25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RelativePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=10000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.pe = self._create_relative_positional_encoding()\n",
        "\n",
        "    def _create_relative_positional_encoding(self):\n",
        "        # Implementation of relative positional encoding for extended context\n",
        "        position = torch.arange(0, self.max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * -(math.log(10000.0) / self.d_model))\n",
        "        pe = torch.zeros(self.max_len, self.d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply relative positional encoding\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "        relative_positions = torch.arange(-seq_len+1, seq_len).unsqueeze(0).expand(batch_size, -1)\n",
        "        relative_positions = torch.clamp(relative_positions, 0, self.max_len-1)\n",
        "        return x + self.pe[relative_positions]\n",
        "\n",
        "class ExtendedAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.o_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None, rel_pos=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Project to multi-head\n",
        "        q = self.q_proj(q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(k).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(v).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention with relative positional encoding\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Add relative positional bias if provided\n",
        "        if rel_pos is not None:\n",
        "            scores = scores + rel_pos\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attention, v)\n",
        "\n",
        "        # Reshape back to batch_size x seq_len x d_model\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.o_proj(context)\n",
        "\n",
        "        return output, attention\n",
        "\n",
        "class HierarchicalMemoryBlock(nn.Module):\n",
        "    def __init__(self, d_model, memory_size=1000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.memory_size = memory_size\n",
        "\n",
        "        # Memory stores\n",
        "        self.active_memory = nn.Parameter(torch.zeros(memory_size, d_model))\n",
        "        self.short_term_memory = nn.Parameter(torch.zeros(memory_size, d_model))\n",
        "        self.long_term_memory = nn.Parameter(torch.zeros(memory_size, d_model))\n",
        "\n",
        "        # Query projections\n",
        "        self.query_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Memory compression\n",
        "        self.compressor = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model // 2, d_model)\n",
        "        )\n",
        "\n",
        "    def store_memory(self, input_data, memory_type=\"active\"):\n",
        "        # Compress and store new information\n",
        "        compressed = self.compressor(input_data)\n",
        "\n",
        "        if memory_type == \"active\":\n",
        "            # Shift active memory and add new data\n",
        "            self.active_memory = torch.cat([compressed, self.active_memory[:-compressed.size(0)]], dim=0)\n",
        "        elif memory_type == \"short_term\":\n",
        "            # Store important information from active to short-term\n",
        "            importance = self._calculate_importance(input_data)\n",
        "            mask = importance > 0.5  # Threshold for importance\n",
        "            self.short_term_memory = torch.cat(\n",
        "                [compressed[mask], self.short_term_memory[:-torch.sum(mask).item()]], dim=0\n",
        "            )\n",
        "        elif memory_type == \"long_term\":\n",
        "            # Store critical information to long-term\n",
        "            importance = self._calculate_importance(input_data)\n",
        "            mask = importance > 0.8  # Higher threshold for long-term\n",
        "            self.long_term_memory = torch.cat(\n",
        "                [compressed[mask], self.long_term_memory[:-torch.sum(mask).item()]], dim=0\n",
        "            )\n",
        "\n",
        "    def _calculate_importance(self, data):\n",
        "        # Calculate importance score for memory items\n",
        "        # This could be based on emotional salience, novelty, etc.\n",
        "        norm = torch.norm(data, dim=-1)\n",
        "        return torch.sigmoid(norm - 5.0)  # Example threshold\n",
        "\n",
        "    def retrieve_memory(self, query, top_k=5):\n",
        "        # Project query\n",
        "        query_proj = self.query_proj(query)\n",
        "\n",
        "        # Calculate relevance scores\n",
        "        active_scores = F.cosine_similarity(query_proj.unsqueeze(1), self.active_memory.unsqueeze(0), dim=-1)\n",
        "        short_term_scores = F.cosine_similarity(query_proj.unsqueeze(1), self.short_term_memory.unsqueeze(0), dim=-1)\n",
        "        long_term_scores = F.cosine_similarity(query_proj.unsqueeze(1), self.long_term_memory.unsqueeze(0), dim=-1)\n",
        "\n",
        "        # Combine scores with decreasing weights for further memories\n",
        "        combined_memories = torch.cat([\n",
        "            self.active_memory * 1.0,\n",
        "            self.short_term_memory * 0.7,\n",
        "            self.long_term_memory * 0.5\n",
        "        ], dim=0)\n",
        "\n",
        "        combined_scores = torch.cat([\n",
        "            active_scores * 1.0,\n",
        "            short_term_scores * 0.7,\n",
        "            long_term_scores * 0.5\n",
        "        ], dim=1)\n",
        "\n",
        "        # Get top-k memories\n",
        "        top_k_scores, top_k_indices = torch.topk(combined_scores, k=top_k, dim=1)\n",
        "        top_k_weights = F.softmax(top_k_scores, dim=1)\n",
        "\n",
        "        # Retrieve and weight memories\n",
        "        batch_size = query.size(0)\n",
        "        retrieved_memories = torch.zeros(batch_size, self.d_model, device=query.device)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            selected_memories = combined_memories[top_k_indices[i]]\n",
        "            retrieved_memories[i] = torch.sum(selected_memories * top_k_weights[i].unsqueeze(1), dim=0)\n",
        "\n",
        "        return retrieved_memories"
      ],
      "metadata": {
        "id": "aiDsOnPGN5Nk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tranining Architecture\n"
      ],
      "metadata": {
        "id": "e6FAuuHJN_8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiObjectiveLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, beta=0.5, gamma=0.3):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # Weight for language modeling\n",
        "        self.beta = beta    # Weight for context retention\n",
        "        self.gamma = gamma  # Weight for conversation structure\n",
        "\n",
        "    def forward(self, lm_loss, context_loss, conv_loss):\n",
        "        return self.alpha * lm_loss + self.beta * context_loss + self.gamma * conv_loss\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Get data\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        context_ids = batch[\"context_ids\"].to(device)  # Additional context information\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            context_ids=context_ids\n",
        "        )\n",
        "\n",
        "        # Calculate losses\n",
        "        lm_loss = F.cross_entropy(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
        "        context_loss = calculate_context_retention_loss(outputs.context_predictions, batch[\"context_labels\"].to(device))\n",
        "        conv_loss = calculate_conversation_structure_loss(outputs.conv_predictions, batch[\"conv_labels\"].to(device))\n",
        "\n",
        "        # Combined loss\n",
        "        criterion = MultiObjectiveLoss(alpha=1.0, beta=0.5, gamma=0.3)\n",
        "        loss = criterion(lm_loss, context_loss, conv_loss)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Distributed training setup\n",
        "def setup_distributed_training(model, world_size):\n",
        "    \"\"\"Set up model for distributed training\"\"\"\n",
        "    # Model parallelism - split layers across GPUs\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model)\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    return model, scaler"
      ],
      "metadata": {
        "id": "MaQy6NxXOH_U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emotional Intellegence Componenet"
      ],
      "metadata": {
        "id": "tI49zmSLOYsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionRecognitionModule(nn.Module):\n",
        "    def __init__(self, d_model, num_emotions=8):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_emotions = num_emotions\n",
        "\n",
        "        # Token-level emotion detection\n",
        "        self.token_emotion = nn.Linear(d_model, num_emotions)\n",
        "\n",
        "        # Utterance-level aggregation\n",
        "        self.query_emotion = nn.Parameter(torch.randn(d_model))\n",
        "\n",
        "        # Conversation-level tracking\n",
        "        self.emotion_gru = nn.GRU(num_emotions, num_emotions, batch_first=True)\n",
        "\n",
        "        # Dimensional emotion representation (valence, arousal, dominance)\n",
        "        self.emotion_dimensions = nn.Linear(d_model, 3)\n",
        "\n",
        "    def forward(self, hidden_states, utterance_boundaries=None):\n",
        "        batch_size, seq_len = hidden_states.size(0), hidden_states.size(1)\n",
        "\n",
        "        # Token-level emotion\n",
        "        token_emotions = self.token_emotion(hidden_states)  # [batch, seq, num_emotions]\n",
        "\n",
        "        # Utterance-level emotion with attention\n",
        "        query = self.query_emotion.expand(batch_size, 1, self.d_model)\n",
        "        attention_scores = torch.bmm(query, hidden_states.transpose(1, 2)).squeeze(1)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(1)\n",
        "        utterance_emotion = torch.bmm(attention_weights, token_emotions).squeeze(1)\n",
        "\n",
        "        # Split by utterance boundaries if provided\n",
        "        if utterance_boundaries is not None:\n",
        "            utterance_emotions = []\n",
        "            for b in range(batch_size):\n",
        "                # Extract utterances based on boundaries\n",
        "                boundaries = utterance_boundaries[b]\n",
        "                batch_utterances = []\n",
        "\n",
        "                for i in range(len(boundaries) - 1):\n",
        "                    start, end = boundaries[i], boundaries[i+1]\n",
        "                    # Average emotions for this utterance\n",
        "                    utterance_avg = token_emotions[b, start:end].mean(dim=0)\n",
        "                    batch_utterances.append(utterance_avg)\n",
        "\n",
        "                utterance_emotions.append(torch.stack(batch_utterances))\n",
        "\n",
        "            # Pad to same length\n",
        "            max_utterances = max(len(u) for u in utterance_emotions)\n",
        "            padded_emotions = torch.zeros(batch_size, max_utterances, self.num_emotions, device=hidden_states.device)\n",
        "            for b, emotions in enumerate(utterance_emotions):\n",
        "                padded_emotions[b, :len(emotions)] = emotions\n",
        "\n",
        "            # Conversation-level emotion tracking with GRU\n",
        "            conversation_emotions, _ = self.emotion_gru(padded_emotions)\n",
        "        else:\n",
        "            # If no boundaries, treat sequence as one utterance\n",
        "            conversation_emotions = utterance_emotion.unsqueeze(1)\n",
        "\n",
        "        # Dimensional emotion representation\n",
        "        emotion_dims = self.emotion_dimensions(hidden_states)  # [batch, seq, 3]\n",
        "        valence, arousal, dominance = emotion_dims.chunk(3, dim=-1)\n",
        "\n",
        "        return {\n",
        "            \"token_emotions\": token_emotions,\n",
        "            \"utterance_emotion\": utterance_emotion,\n",
        "            \"conversation_emotions\": conversation_emotions,\n",
        "            \"emotion_dimensions\": {\n",
        "                \"valence\": valence.squeeze(-1),\n",
        "                \"arousal\": arousal.squeeze(-1),\n",
        "                \"dominance\": dominance.squeeze(-1)\n",
        "            }\n",
        "        }"
      ],
      "metadata": {
        "id": "a_arE9HROfDd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emotion Response Generation"
      ],
      "metadata": {
        "id": "rTsHQbZBOlOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionalResponseGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, emotion_dim=8):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emotion_dim = emotion_dim\n",
        "\n",
        "        # Emotion conditioning layer\n",
        "        self.emotion_conditioning = nn.Linear(emotion_dim, d_model)\n",
        "\n",
        "        # Vocabulary biasing\n",
        "        self.emotion_vocab_bias = nn.Linear(emotion_dim, vocab_size)\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, hidden_states, target_emotion):\n",
        "        batch_size, seq_len = hidden_states.size(0), hidden_states.size(1)\n",
        "\n",
        "        # Condition hidden states on target emotion\n",
        "        emotion_features = self.emotion_conditioning(target_emotion).unsqueeze(1)\n",
        "        conditioned_states = hidden_states + emotion_features\n",
        "\n",
        "        # Generate base logits\n",
        "        base_logits = self.output_projection(conditioned_states)\n",
        "\n",
        "        # Apply emotion-specific vocabulary bias\n",
        "        emotion_bias = self.emotion_vocab_bias(target_emotion).unsqueeze(1)\n",
        "        biased_logits = base_logits + 0.1 * emotion_bias  # Scale factor for bias\n",
        "\n",
        "        return biased_logits\n",
        "\n",
        "    def generate_appropriate_emotion(self, user_emotion, context_state):\n",
        "        # Calculate appropriate emotional response\n",
        "        # - Mirror user emotion with dampening for negative emotions\n",
        "        # - Enhance positive emotions slightly\n",
        "        valence = user_emotion[:, 0]  # Assuming first dimension is valence\n",
        "\n",
        "        # If negative emotion, respond with more positive but understanding emotion\n",
        "        response_valence = torch.where(\n",
        "            valence < 0,\n",
        "            valence * 0.5 + 0.2,  # Dampen negative, shift positive\n",
        "            valence * 1.1          # Slightly enhance positive\n",
        "        )\n",
        "\n",
        "        # For other dimensions (arousal, dominance), match with moderation\n",
        "        response_emotion = user_emotion.clone()\n",
        "        response_emotion[:, 0] = response_valence\n",
        "        response_emotion[:, 1] = user_emotion[:, 1] * 0.8  # Moderate arousal\n",
        "\n",
        "        return response_emotion"
      ],
      "metadata": {
        "id": "ZQwrmfxYOrB9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Conversational Initiative System\n",
        " Initiative Detection\n",
        ""
      ],
      "metadata": {
        "id": "JuYIoswROuLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InitiativeSystem(nn.Module):\n",
        "    def __init__(self, d_model, d_hidden=128):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Initiative prediction network\n",
        "        self.initiative_predictor = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Initiative type classifier\n",
        "        self.initiative_type = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden, 4)  # 4 types: question, suggestion, follow-up, info-sharing\n",
        "        )\n",
        "\n",
        "        # Initiative threshold parameters\n",
        "        self.base_threshold = 0.7\n",
        "        self.decay_rate = 0.1\n",
        "\n",
        "    def should_take_initiative(self, context_state, time_since_last_message):\n",
        "        # Calculate initiative score\n",
        "        initiative_score = self.initiative_predictor(context_state).squeeze(-1)\n",
        "\n",
        "        # Calculate adaptive threshold based on time\n",
        "        threshold = self.base_threshold * (1.0 - torch.exp(-self.decay_rate * time_since_last_message))\n",
        "\n",
        "        # Decision to take initiative\n",
        "        should_initiate = initiative_score > threshold\n",
        "\n",
        "        # If initiative should be taken, determine type\n",
        "        initiative_type_logits = self.initiative_type(context_state)\n",
        "        initiative_type = torch.argmax(initiative_type_logits, dim=-1)\n",
        "\n",
        "        return should_initiate, initiative_type\n",
        "\n",
        "    def calculate_follow_up_score(self, project_info):\n",
        "        # Calculate when to follow up on projects/activities\n",
        "        importance = project_info['importance']\n",
        "        time_since_mention = project_info['time_since_last_mentioned']\n",
        "        status = project_info['status_factor']\n",
        "\n",
        "        # Higher score means higher priority for follow-up\n",
        "        follow_up_score = importance * (time_since_mention + 1) * status\n",
        "\n",
        "        return follow_up_score"
      ],
      "metadata": {
        "id": "5ne1xgqbOzOc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project and Activity Tracking"
      ],
      "metadata": {
        "id": "3-SLmLzKO8_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectTracker:\n",
        "    def __init__(self):\n",
        "        self.projects = {}\n",
        "        self.current_time = 0\n",
        "\n",
        "    def update_time(self, increment=1):\n",
        "        self.current_time += increment\n",
        "\n",
        "    def add_project(self, project_id, name, details=\"\", importance=0.5, status=\"active\"):\n",
        "        \"\"\"Add a new project to tracking\"\"\"\n",
        "        self.projects[project_id] = {\n",
        "            \"id\": project_id,\n",
        "            \"name\": name,\n",
        "            \"last_mentioned\": self.current_time,\n",
        "            \"status\": status,\n",
        "            \"details\": details,\n",
        "            \"importance\": importance,\n",
        "            \"follow_up_schedule\": self.current_time + int(10 / importance)  # More important = earlier follow-up\n",
        "        }\n",
        "\n",
        "    def update_project(self, project_id, **kwargs):\n",
        "        \"\"\"Update project information\"\"\"\n",
        "        if project_id in self.projects:\n",
        "            for key, value in kwargs.items():\n",
        "                if key in self.projects[project_id]:\n",
        "                    self.projects[project_id][key] = value\n",
        "\n",
        "            # Auto-update last_mentioned\n",
        "            self.projects[project_id][\"last_mentioned\"] = self.current_time\n",
        "\n",
        "    def get_projects_for_follow_up(self):\n",
        "        \"\"\"Get projects that need follow-up\"\"\"\n",
        "        follow_up_candidates = []\n",
        "\n",
        "        for project_id, project in self.projects.items():\n",
        "            if project[\"status\"] != \"completed\":\n",
        "                # Calculate follow-up score\n",
        "                time_since_mention = self.current_time - project[\"last_mentioned\"]\n",
        "                status_factor = 1.5 if project[\"status\"] == \"blocked\" else 1.0\n",
        "\n",
        "                score = project[\"importance\"] * time_since_mention * status_factor\n",
        "\n",
        "                if score > 5.0:  # Threshold for follow-up\n",
        "                    follow_up_candidates.append((project_id, score))\n",
        "\n",
        "        # Sort by score\n",
        "        follow_up_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return [self.projects[pid] for pid, _ in follow_up_candidates]\n",
        "\n",
        "    def extract_project_info(self, text):\n",
        "        \"\"\"Extract project information from text (simplified)\"\"\"\n",
        "        # In a real implementation, this would use NER and relation extraction\n",
        "        # This is a placeholder implementation\n",
        "\n",
        "        # Simple keyword matching\n",
        "        project_keywords = [\"project\", \"task\", \"work on\", \"assignment\"]\n",
        "\n",
        "        detected_projects = []\n",
        "        # Simplified detection logic\n",
        "        if any(keyword in text.lower() for keyword in project_keywords):\n",
        "            # Extract project name (simplified)\n",
        "            words = text.split()\n",
        "            for i, word in enumerate(words):\n",
        "                if word.lower() in project_keywords and i+1 < len(words):\n",
        "                    project_name = words[i+1]\n",
        "\n",
        "                    # Generate ID\n",
        "                    project_id = f\"proj_{len(self.projects)}\"\n",
        "\n",
        "                    # Estimate importance (simplified)\n",
        "                    importance_words = [\"urgent\", \"important\", \"critical\", \"crucial\"]\n",
        "                    importance = 0.5  # Default\n",
        "                    for imp_word in importance_words:\n",
        "                        if imp_word in text.lower():\n",
        "                            importance = 0.8\n",
        "                            break\n",
        "\n",
        "                    detected_projects.append({\n",
        "                        \"id\": project_id,\n",
        "                        \"name\": project_name,\n",
        "                        \"importance\": importance\n",
        "                    })\n",
        "\n",
        "        return detected_projects"
      ],
      "metadata": {
        "id": "Lis_oBEPPAw5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integration Architecture"
      ],
      "metadata": {
        "id": "V0P0TrSVPK0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextAwareEmotionalLLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        d_model=768,\n",
        "        n_layers=12,\n",
        "        n_heads=12,\n",
        "        max_seq_len=2048,\n",
        "        emotion_dim=8\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layers\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = RelativePositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        # Transformer layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model, n_heads) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Memory system\n",
        "        self.memory = HierarchicalMemoryBlock(d_model)\n",
        "\n",
        "        # Emotion system\n",
        "        self.emotion_recognition = EmotionRecognitionModule(d_model, emotion_dim)\n",
        "        self.emotional_generator = EmotionalResponseGenerator(vocab_size, d_model, emotion_dim)\n",
        "\n",
        "        # Initiative system\n",
        "        self.initiative_system = InitiativeSystem(d_model)\n",
        "\n",
        "        # Project tracking\n",
        "        self.project_tracker = ProjectTracker()\n",
        "\n",
        "        # Output layer\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, context_ids=None):\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        # Embeddings\n",
        "        token_embeds = self.token_embedding(input_ids)\n",
        "        position_embeds = self.positional_encoding(token_embeds)\n",
        "        hidden_states = token_embeds + position_embeds\n",
        "\n",
        "        # Process through transformer layers\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states, attention_mask)\n",
        "\n",
        "        # Retrieve from memory if context provided\n",
        "        if context_ids is not None:\n",
        "            context_embeds = self.token_embedding(context_ids)\n",
        "            memory_states = self.memory.retrieve_memory(context_embeds)\n",
        "            hidden_states = hidden_states + memory_states\n",
        "\n",
        "        # Recognize emotions\n",
        "        emotion_data = self.emotion_recognition(hidden_states)\n",
        "        user_emotion = emotion_data[\"utterance_emotion\"]\n",
        "\n",
        "        # Generate appropriate emotional response\n",
        "        target_emotion = self.emotional_generator.generate_appropriate_emotion(\n",
        "            user_emotion, hidden_states[:, -1]\n",
        "        )\n",
        "\n",
        "        # Check for initiative\n",
        "        time_since_last = torch.ones(batch_size)  # Placeholder, would come from context\n",
        "        should_initiate, initiative_type = self.initiative_system.should_take_initiative(\n",
        "            hidden_states[:, -1], time_since_last\n",
        "        )\n",
        "\n",
        "        # Generate output logits with emotional conditioning\n",
        "        logits = self.emotional_generator(hidden_states, target_emotion)\n",
        "\n",
        "        # Store important information in memory\n",
        "        self.memory.store_memory(hidden_states[:, -1].detach(), \"active\")\n",
        "\n",
        "        # Update project tracking from input\n",
        "        for b in range(batch_size):\n",
        "            text = \"Sample text\"  # Would decode from input_ids\n",
        "            projects = self.project_tracker.extract_project_info(text)\n",
        "            for project in projects:\n",
        "                self.project_tracker.add_project(**project)\n",
        "\n",
        "        return {\n",
        "            \"logits\": logits,\n",
        "            \"emotion_data\": emotion_data,\n",
        "            \"target_emotion\": target_emotion,\n",
        "            \"initiative\": {\n",
        "                \"should_initiate\": should_initiate,\n",
        "                \"initiative_type\": initiative_type\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.7, do_sample=True):\n",
        "        \"\"\"Simple generation function\"\"\"\n",
        "        batch_size = input_ids.size(0)\n",
        "        current_ids = input_ids.clone()\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass\n",
        "            outputs = self.forward(current_ids)\n",
        "            next_token_logits = outputs[\"logits\"][:, -1, :]\n",
        "\n",
        "            # Apply temperature\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "\n",
        "            # Sample or greedy\n",
        "            if do_sample:\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "            # Concatenate new tokens\n",
        "            current_ids = torch.cat([current_ids, next_token], dim=1)\n",
        "\n",
        "            # Check for EOS\n",
        "            if (next_token == eos_token_id).all():\n",
        "                break\n",
        "\n",
        "        return current_ids"
      ],
      "metadata": {
        "id": "U22iq-WkPLwi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Optimization Framework\n",
        "\n",
        "Multi-Objective Training Strategy"
      ],
      "metadata": {
        "id": "beB9MdB1PQaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_framework(model, train_dataloader, val_dataloader, num_epochs=10):\n",
        "    # Optimizer setup\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    total_steps = len(train_dataloader) * num_epochs\n",
        "    warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
        "\n",
        "        # Validate\n",
        "        val_loss = validate_model(model, val_dataloader, device)\n",
        "\n",
        "        # Track metrics\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint if improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_checkpoint(model, optimizer, scheduler, epoch, val_loss)\n",
        "\n",
        "    # Final evaluation\n",
        "    test_metrics = evaluate_model(model, test_dataloader, device)\n",
        "\n",
        "    return model, test_metrics"
      ],
      "metadata": {
        "id": "W_z7IjjAPYgJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preference Learning and Reinforcement"
      ],
      "metadata": {
        "id": "7MZ1JQVWPbAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModel(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.reward_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # Extract CLS token or use mean pooling\n",
        "        pooled = hidden_states.mean(dim=1)\n",
        "        reward = self.reward_head(pooled)\n",
        "        return reward\n",
        "\n",
        "def train_with_dpo(model, optimizer, dataloader, reward_model, beta=0.1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Get preferred and rejected responses for each context\n",
        "        contexts = batch[\"contexts\"].to(device)\n",
        "        preferred = batch[\"preferred_responses\"].to(device)\n",
        "        rejected = batch[\"rejected_responses\"].to(device)\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.no_grad():\n",
        "            # Generate log probs for current model\n",
        "            preferred_outputs = model(contexts, preferred)\n",
        "            rejected_outputs = model(contexts, rejected)\n",
        "\n",
        "            # Get reward scores from reward model\n",
        "            preferred_rewards = reward_model(preferred_outputs.hidden_states).squeeze(-1)\n",
        "            rejected_rewards = reward_model(rejected_outputs.hidden_states).squeeze(-1)\n",
        "\n",
        "        # Calculate DPO loss\n",
        "        # L_DPO(θ) = -E[(log σ(β(r_θ(x,y_w) - r_θ(x,y_l)))]\n",
        "        reward_diff = preferred_rewards - rejected_rewards\n",
        "        loss = -torch.log(torch.sigmoid(beta * reward_diff)).mean()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "qzR7p5E5PhM8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continual Learning Framework"
      ],
      "metadata": {
        "id": "4hkWgUmiPnqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import copy\n",
        "from typing import Dict, List, Tuple, Optional, Union, Callable\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ContinualLearningFramework:\n",
        "    \"\"\"\n",
        "    A framework for implementing various continual learning methods.\n",
        "    This framework supports:\n",
        "    - Experience Replay\n",
        "    - Elastic Weight Consolidation (EWC)\n",
        "    - Knowledge Distillation\n",
        "    - Task-specific adapters\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer_class: torch.optim.Optimizer = optim.Adam,\n",
        "        optimizer_kwargs: Dict = {'lr': 0.001},\n",
        "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "        replay_buffer_size: int = 1000,\n",
        "        use_ewc: bool = False,\n",
        "        ewc_lambda: float = 100.0,\n",
        "        use_distillation: bool = False,\n",
        "        distillation_temp: float = 2.0,\n",
        "        distillation_alpha: float = 0.5,\n",
        "        use_adapters: bool = False,\n",
        "        adapter_size: int = 64\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the continual learning framework.\n",
        "\n",
        "        Args:\n",
        "            model: The neural network model\n",
        "            optimizer_class: The optimizer class to use\n",
        "            optimizer_kwargs: Arguments for the optimizer\n",
        "            device: Device to run the model on\n",
        "            replay_buffer_size: Size of experience replay buffer\n",
        "            use_ewc: Whether to use Elastic Weight Consolidation\n",
        "            ewc_lambda: EWC regularization strength\n",
        "            use_distillation: Whether to use knowledge distillation\n",
        "            distillation_temp: Temperature for distillation\n",
        "            distillation_alpha: Weight for distillation loss\n",
        "            use_adapters: Whether to use task-specific adapters\n",
        "            adapter_size: Size of adapter hidden representations\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer_class = optimizer_class\n",
        "        self.optimizer_kwargs = optimizer_kwargs\n",
        "        self.device = device\n",
        "        self.current_task_id = None\n",
        "        self.seen_tasks = set()\n",
        "\n",
        "        # Experience Replay\n",
        "        self.replay_buffer_size = replay_buffer_size\n",
        "        self.replay_buffer = {}  # task_id -> [(x, y), ...]\n",
        "\n",
        "        # EWC\n",
        "        self.use_ewc = use_ewc\n",
        "        self.ewc_lambda = ewc_lambda\n",
        "        self.fisher_dict = {}  # task_id -> {param_name: fisher}\n",
        "        self.param_dict = {}   # task_id -> {param_name: param_value}\n",
        "\n",
        "        # Knowledge Distillation\n",
        "        self.use_distillation = use_distillation\n",
        "        self.distillation_temp = distillation_temp\n",
        "        self.distillation_alpha = distillation_alpha\n",
        "        self.teacher_model = None\n",
        "\n",
        "        # Task-specific Adapters\n",
        "        self.use_adapters = use_adapters\n",
        "        self.adapter_size = adapter_size\n",
        "        self.adapters = nn.ModuleDict()  # task_id -> adapter\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = self.optimizer_class(self.model.parameters(), **self.optimizer_kwargs)\n",
        "\n",
        "        logger.info(f\"Initialized Continual Learning Framework on device: {device}\")\n",
        "        logger.info(f\"Using EWC: {use_ewc}, Distillation: {use_distillation}, Adapters: {use_adapters}\")\n",
        "\n",
        "    def _compute_fisher_information(self, data_loader: DataLoader, num_samples: int = 100) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Compute the Fisher information matrix for parameters.\n",
        "\n",
        "        Args:\n",
        "            data_loader: DataLoader with the current task data\n",
        "            num_samples: Number of samples to use for estimation\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with parameter names and their Fisher information\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "\n",
        "        logger.info(f\"Computing Fisher information matrix using {num_samples} samples\")\n",
        "\n",
        "        sample_count = 0\n",
        "        for data_batch in data_loader:\n",
        "            if sample_count >= num_samples:\n",
        "                break\n",
        "\n",
        "            inputs, targets = data_batch\n",
        "            inputs = inputs.to(self.device)\n",
        "            targets = targets.to(self.device)\n",
        "            batch_size = inputs.shape[0]\n",
        "\n",
        "            self.model.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "\n",
        "            # For each sample, compute the fisher information\n",
        "            for i in range(min(batch_size, num_samples - sample_count)):\n",
        "                log_prob = torch.nn.functional.log_softmax(outputs[i:i+1], dim=1)\n",
        "                loss = -torch.sum(torch.exp(log_prob) * log_prob)  # -sum(p * log_p)\n",
        "                loss.backward(retain_graph=(i < batch_size - 1))\n",
        "\n",
        "                # Accumulate the gradients\n",
        "                for n, p in self.model.named_parameters():\n",
        "                    if p.grad is not None and p.requires_grad:\n",
        "                        fisher[n] += p.grad.pow(2).detach() / num_samples\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                sample_count += 1\n",
        "\n",
        "        logger.info(f\"Fisher information matrix computed with {sample_count} samples\")\n",
        "        return fisher\n",
        "\n",
        "    def _save_current_parameters(self) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Save the current parameters of the model.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with parameter names and their values\n",
        "        \"\"\"\n",
        "        return {n: p.detach().clone() for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "\n",
        "    def _add_to_replay_buffer(self, task_id: str, data: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
        "        \"\"\"\n",
        "        Add data to the replay buffer for a given task.\n",
        "\n",
        "        Args:\n",
        "            task_id: Identifier for the task\n",
        "            data: List of (input, target) tuples\n",
        "        \"\"\"\n",
        "        if task_id not in self.replay_buffer:\n",
        "            self.replay_buffer[task_id] = []\n",
        "\n",
        "        self.replay_buffer[task_id].extend(data)\n",
        "\n",
        "        # Limit the buffer size by randomly sampling if needed\n",
        "        if len(self.replay_buffer[task_id]) > self.replay_buffer_size:\n",
        "            indices = np.random.choice(\n",
        "                len(self.replay_buffer[task_id]),\n",
        "                self.replay_buffer_size,\n",
        "                replace=False\n",
        "            )\n",
        "            self.replay_buffer[task_id] = [self.replay_buffer[task_id][i] for i in indices]\n",
        "\n",
        "        logger.info(f\"Added data to replay buffer for task {task_id}, buffer size: {len(self.replay_buffer[task_id])}\")\n",
        "\n",
        "    def _create_adapter(self, task_id: str, input_dim: int, output_dim: int):\n",
        "        \"\"\"\n",
        "        Create a task-specific adapter module.\n",
        "\n",
        "        Args:\n",
        "            task_id: Task identifier\n",
        "            input_dim: Input dimension of the adapter\n",
        "            output_dim: Output dimension of the adapter\n",
        "        \"\"\"\n",
        "        adapter = nn.Sequential(\n",
        "            nn.Linear(input_dim, self.adapter_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.adapter_size, output_dim)\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.adapters[task_id] = adapter\n",
        "        logger.info(f\"Created adapter for task {task_id}\")\n",
        "\n",
        "    def start_task(self, task_id: str):\n",
        "        \"\"\"\n",
        "        Start training on a new task.\n",
        "\n",
        "        Args:\n",
        "            task_id: Identifier for the task\n",
        "        \"\"\"\n",
        "        self.current_task_id = task_id\n",
        "\n",
        "        # If this is an existing task, load its adapter\n",
        "        if self.use_adapters and task_id in self.adapters:\n",
        "            logger.info(f\"Switching to existing adapter for task {task_id}\")\n",
        "\n",
        "        # Save teacher model for distillation if needed\n",
        "        if self.use_distillation and task_id not in self.seen_tasks:\n",
        "            logger.info(\"Saving teacher model for knowledge distillation\")\n",
        "            self.teacher_model = copy.deepcopy(self.model)\n",
        "            self.teacher_model.eval()\n",
        "\n",
        "        # Create a new optimizer for the task\n",
        "        if self.use_adapters and task_id in self.adapters:\n",
        "            # Only optimize the adapter parameters for existing tasks\n",
        "            self.optimizer = self.optimizer_class(\n",
        "                self.adapters[task_id].parameters(),\n",
        "                **self.optimizer_kwargs\n",
        "            )\n",
        "        else:\n",
        "            # Optimize all parameters for new tasks\n",
        "            self.optimizer = self.optimizer_class(\n",
        "                self.model.parameters(),\n",
        "                **self.optimizer_kwargs\n",
        "            )\n",
        "\n",
        "        logger.info(f\"Started task {task_id}\")\n",
        "\n",
        "    def train_step(\n",
        "        self,\n",
        "        inputs: torch.Tensor,\n",
        "        targets: torch.Tensor,\n",
        "        task_specific_loss_fn: Callable = nn.CrossEntropyLoss()\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Perform a single training step.\n",
        "\n",
        "        Args:\n",
        "            inputs: Input batch\n",
        "            targets: Target batch\n",
        "            task_specific_loss_fn: Loss function specific to the current task\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with loss metrics\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        # Move data to device\n",
        "        inputs = inputs.to(self.device)\n",
        "        targets = targets.to(self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "\n",
        "        # Task-specific loss\n",
        "        task_loss = task_specific_loss_fn(outputs, targets)\n",
        "        total_loss = task_loss\n",
        "        loss_metrics = {\"task_loss\": task_loss.item()}\n",
        "\n",
        "        # Knowledge distillation loss\n",
        "        if self.use_distillation and self.teacher_model is not None:\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = self.teacher_model(inputs)\n",
        "\n",
        "            # Compute distillation loss\n",
        "            distillation_loss = self._compute_distillation_loss(\n",
        "                outputs, teacher_outputs, self.distillation_temp\n",
        "            )\n",
        "\n",
        "            # Combine losses\n",
        "            total_loss = (\n",
        "                (1 - self.distillation_alpha) * task_loss +\n",
        "                self.distillation_alpha * distillation_loss\n",
        "            )\n",
        "            loss_metrics[\"distillation_loss\"] = distillation_loss.item()\n",
        "\n",
        "        # EWC loss\n",
        "        if self.use_ewc and self.fisher_dict and self.param_dict:\n",
        "            ewc_loss = self._compute_ewc_loss()\n",
        "            total_loss += self.ewc_lambda * ewc_loss\n",
        "            loss_metrics[\"ewc_loss\"] = ewc_loss.item()\n",
        "\n",
        "        # Backward pass and optimizer step\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        loss_metrics[\"total_loss\"] = total_loss.item()\n",
        "        return loss_metrics\n",
        "\n",
        "    def _compute_distillation_loss(\n",
        "        self,\n",
        "        outputs: torch.Tensor,\n",
        "        teacher_outputs: torch.Tensor,\n",
        "        temperature: float\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the knowledge distillation loss.\n",
        "\n",
        "        Args:\n",
        "            outputs: Model outputs\n",
        "            teacher_outputs: Teacher model outputs\n",
        "            temperature: Softmax temperature\n",
        "\n",
        "        Returns:\n",
        "            Distillation loss\n",
        "        \"\"\"\n",
        "        soft_targets = torch.nn.functional.softmax(teacher_outputs / temperature, dim=1)\n",
        "        log_probs = torch.nn.functional.log_softmax(outputs / temperature, dim=1)\n",
        "        distillation_loss = -torch.sum(soft_targets * log_probs) / outputs.size(0)\n",
        "        return distillation_loss * (temperature ** 2)\n",
        "\n",
        "    def _compute_ewc_loss(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the EWC regularization loss.\n",
        "\n",
        "        Returns:\n",
        "            EWC loss\n",
        "        \"\"\"\n",
        "        ewc_loss = 0\n",
        "        for task_id in self.seen_tasks:\n",
        "            if task_id in self.fisher_dict and task_id in self.param_dict:\n",
        "                for n, p in self.model.named_parameters():\n",
        "                    if n in self.fisher_dict[task_id] and n in self.param_dict[task_id]:\n",
        "                        fisher = self.fisher_dict[task_id][n]\n",
        "                        old_param = self.param_dict[task_id][n]\n",
        "                        ewc_loss += torch.sum(fisher * (p - old_param) ** 2)\n",
        "        return ewc_loss\n",
        "\n",
        "    def train_on_task(\n",
        "        self,\n",
        "        task_id: str,\n",
        "        data_loader: DataLoader,\n",
        "        num_epochs: int,\n",
        "        task_specific_loss_fn: Callable = nn.CrossEntropyLoss(),\n",
        "        validation_loader: Optional[DataLoader] = None,\n",
        "        early_stopping_patience: Optional[int] = None,\n",
        "        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None\n",
        "    ) -> Dict[str, List[float]]:\n",
        "        \"\"\"\n",
        "        Train the model on a specific task.\n",
        "\n",
        "        Args:\n",
        "            task_id: Task identifier\n",
        "            data_loader: DataLoader with task data\n",
        "            num_epochs: Number of training epochs\n",
        "            task_specific_loss_fn: Task-specific loss function\n",
        "            validation_loader: Optional validation data loader\n",
        "            early_stopping_patience: Patience for early stopping\n",
        "            scheduler: Optional learning rate scheduler\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with training metrics\n",
        "        \"\"\"\n",
        "        # Start the task\n",
        "        self.start_task(task_id)\n",
        "        logger.info(f\"Training on task {task_id} for {num_epochs} epochs\")\n",
        "\n",
        "        # Initialize metrics\n",
        "        metrics = {\n",
        "            \"train_loss\": [],\n",
        "            \"val_loss\": [] if validation_loader else None\n",
        "        }\n",
        "\n",
        "        # Early stopping variables\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_state = None\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            epoch_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            for batch in data_loader:\n",
        "                inputs, targets = batch\n",
        "                loss_metrics = self.train_step(inputs, targets, task_specific_loss_fn)\n",
        "                epoch_loss += loss_metrics[\"total_loss\"]\n",
        "                num_batches += 1\n",
        "\n",
        "                # Add samples to replay buffer\n",
        "                if num_batches % 10 == 0:  # Sample every 10 batches\n",
        "                    batch_samples = [(inputs[i].detach().cpu(), targets[i].detach().cpu())\n",
        "                                    for i in range(min(5, len(inputs)))]\n",
        "                    self._add_to_replay_buffer(task_id, batch_samples)\n",
        "\n",
        "            avg_train_loss = epoch_loss / num_batches\n",
        "            metrics[\"train_loss\"].append(avg_train_loss)\n",
        "\n",
        "            # Validation phase\n",
        "            if validation_loader:\n",
        "                val_loss = self.evaluate(validation_loader, task_specific_loss_fn)\n",
        "                metrics[\"val_loss\"].append(val_loss)\n",
        "\n",
        "                # Early stopping check\n",
        "                if early_stopping_patience and val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    best_model_state = copy.deepcopy(self.model.state_dict())\n",
        "                    patience_counter = 0\n",
        "                elif early_stopping_patience:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= early_stopping_patience:\n",
        "                        logger.info(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
        "                        self.model.load_state_dict(best_model_state)\n",
        "                        break\n",
        "\n",
        "                logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "            else:\n",
        "                logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "            # Learning rate scheduler step\n",
        "            if scheduler:\n",
        "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) and validation_loader:\n",
        "                    scheduler.step(val_loss)\n",
        "                else:\n",
        "                    scheduler.step()\n",
        "\n",
        "        # After training, compute fisher information matrix for EWC if enabled\n",
        "        if self.use_ewc:\n",
        "            logger.info(f\"Computing Fisher information matrix for task {task_id}\")\n",
        "            self.fisher_dict[task_id] = self._compute_fisher_information(data_loader)\n",
        "            self.param_dict[task_id] = self._save_current_parameters()\n",
        "\n",
        "        # Create task-specific adapter if enabled\n",
        "        if self.use_adapters and task_id not in self.adapters:\n",
        "            # Assuming the model has a feature extractor and a classifier\n",
        "            # This is a common architecture, but you might need to modify based on your model\n",
        "            if hasattr(self.model, 'fc'):  # For models like ResNet\n",
        "                input_dim = self.model.fc.in_features\n",
        "                output_dim = self.model.fc.out_features\n",
        "                self._create_adapter(task_id, input_dim, output_dim)\n",
        "            elif hasattr(self.model, 'classifier'):  # For models like VGG\n",
        "                # Assuming the last layer is the output layer\n",
        "                classifier = self.model.classifier\n",
        "                if isinstance(classifier, nn.Sequential):\n",
        "                    for module in reversed(classifier):\n",
        "                        if isinstance(module, nn.Linear):\n",
        "                            input_dim = module.in_features\n",
        "                            output_dim = module.out_features\n",
        "                            self._create_adapter(task_id, input_dim, output_dim)\n",
        "                            break\n",
        "\n",
        "        # Add task to seen tasks\n",
        "        self.seen_tasks.add(task_id)\n",
        "        logger.info(f\"Finished training on task {task_id}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        data_loader: DataLoader,\n",
        "        loss_fn: Callable = nn.CrossEntropyLoss()\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate the model on a dataset.\n",
        "\n",
        "        Args:\n",
        "            data_loader: DataLoader with evaluation data\n",
        "            loss_fn: Loss function\n",
        "\n",
        "        Returns:\n",
        "            Average loss\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loader:\n",
        "                inputs, targets = batch\n",
        "                inputs = inputs.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = loss_fn(outputs, targets)\n",
        "\n",
        "                total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Compute accuracy\n",
        "                _, predicted = outputs.max(1)\n",
        "                total_correct += predicted.eq(targets).sum().item()\n",
        "                total_samples += targets.size(0)\n",
        "\n",
        "        avg_loss = total_loss / total_samples\n",
        "        accuracy = total_correct / total_samples\n",
        "\n",
        "        logger.info(f\"Evaluation - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "        return avg_loss\n",
        "\n",
        "    def replay_experiences(\n",
        "        self,\n",
        "        num_samples: int = 100,\n",
        "        task_specific_loss_fn: Callable = nn.CrossEntropyLoss()\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Train the model on experiences from the replay buffer.\n",
        "\n",
        "        Args:\n",
        "            num_samples: Number of samples to replay\n",
        "            task_specific_loss_fn: Loss function\n",
        "        \"\"\"\n",
        "        if not self.replay_buffer:\n",
        "            logger.info(\"No experiences in replay buffer\")\n",
        "            return\n",
        "\n",
        "        # Sample experiences from all tasks\n",
        "        all_experiences = []\n",
        "        for task_id, experiences in self.replay_buffer.items():\n",
        "            if task_id != self.current_task_id:  # Don't replay current task\n",
        "                all_experiences.extend(experiences)\n",
        "\n",
        "        if not all_experiences:\n",
        "            logger.info(\"No past experiences to replay\")\n",
        "            return\n",
        "\n",
        "        # Randomly sample from experiences\n",
        "        if len(all_experiences) > num_samples:\n",
        "            indices = np.random.choice(len(all_experiences), num_samples, replace=False)\n",
        "            sampled_experiences = [all_experiences[i] for i in indices]\n",
        "        else:\n",
        "            sampled_experiences = all_experiences\n",
        "\n",
        "        logger.info(f\"Replaying {len(sampled_experiences)} experiences from past tasks\")\n",
        "\n",
        "        # Train on these experiences\n",
        "        inputs = torch.stack([exp[0] for exp in sampled_experiences]).to(self.device)\n",
        "        targets = torch.stack([exp[1] for exp in sampled_experiences]).to(self.device)\n",
        "\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = task_specific_loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        logger.info(f\"Replay loss: {loss.item():.4f}\")\n",
        "\n",
        "    def save_model(self, path: str):\n",
        "        \"\"\"\n",
        "        Save the continual learning model.\n",
        "\n",
        "        Args:\n",
        "            path: Path to save the model\n",
        "        \"\"\"\n",
        "        save_dict = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'seen_tasks': self.seen_tasks,\n",
        "            'current_task_id': self.current_task_id,\n",
        "            'fisher_dict': self.fisher_dict,\n",
        "            'param_dict': self.param_dict,\n",
        "        }\n",
        "\n",
        "        if self.use_adapters:\n",
        "            save_dict['adapters_state_dict'] = self.adapters.state_dict()\n",
        "\n",
        "        torch.save(save_dict, path)\n",
        "        logger.info(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, path: str):\n",
        "        \"\"\"\n",
        "        Load a saved continual learning model.\n",
        "\n",
        "        Args:\n",
        "            path: Path to the saved model\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.seen_tasks = checkpoint['seen_tasks']\n",
        "        self.current_task_id = checkpoint['current_task_id']\n",
        "        self.fisher_dict = checkpoint['fisher_dict']\n",
        "        self.param_dict = checkpoint['param_dict']\n",
        "\n",
        "        if self.use_adapters and 'adapters_state_dict' in checkpoint:\n",
        "            self.adapters.load_state_dict(checkpoint['adapters_state_dict'])\n",
        "\n",
        "        logger.info(f\"Model loaded from {path}\")\n",
        "\n",
        "# Example of a simple task-specific dataset class\n",
        "class TaskDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for task-specific data.\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            X: Input features\n",
        "            y: Target labels\n",
        "            transform: Optional data transformations\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.y[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# Example usage\n",
        "def example_usage():\n",
        "    \"\"\"\n",
        "    Example of how to use the continual learning framework.\n",
        "    \"\"\"\n",
        "    # Create a simple model\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(784, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 10)\n",
        "    )\n",
        "\n",
        "    # Initialize the framework\n",
        "    cl_framework = ContinualLearningFramework(\n",
        "        model=model,\n",
        "        use_ewc=True,\n",
        "        use_distillation=True,\n",
        "        use_adapters=False\n",
        "    )\n",
        "\n",
        "    # Example tasks (MNIST digits 0-4 and 5-9)\n",
        "    # In a real scenario, you would load your actual task data\n",
        "\n",
        "    # Task 1: Digits 0-4\n",
        "    # task1_data = load_digits_0_to_4()\n",
        "    # task1_dataset = TaskDataset(task1_data.X, task1_data.y)\n",
        "    # task1_loader = DataLoader(task1_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Train on task 1\n",
        "    # cl_framework.train_on_task('digits_0_4', task1_loader, num_epochs=5)\n",
        "\n",
        "    # Task 2: Digits 5-9\n",
        "    # task2_data = load_digits_5_to_9()\n",
        "    # task2_dataset = TaskDataset(task2_data.X, task2_data.y)\n",
        "    # task2_loader = DataLoader(task2_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Train on task 2\n",
        "    # cl_framework.train_on_task('digits_5_9', task2_loader, num_epochs=5)\n",
        "\n",
        "    # Evaluate on both tasks\n",
        "    # cl_framework.evaluate(task1_loader)\n",
        "    # cl_framework.evaluate(task2_loader)\n",
        "\n",
        "    # Save the model\n",
        "    # cl_framework.save_model('continual_learning_model.pt')\n",
        "\n",
        "    logger.info(\"Example usage completed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    example_usage()"
      ],
      "metadata": {
        "id": "fsmd0r6UQx6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}